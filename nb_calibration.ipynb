{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import utils # from The Google Research Authors\n",
    "import torch as t, torch.nn as nn, torch.nn.functional as tnnF, torch.distributions as tdist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "#import ipdb\n",
    "import numpy as np\n",
    "import wideresnet # from The Google Research Authors\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "from tqdm import tqdm\n",
    "t.backends.cudnn.benchmark = True\n",
    "t.backends.cudnn.enabled = True\n",
    "seed = 1\n",
    "\n",
    "# images RGB 32x32\n",
    "im_sz = 32\n",
    "n_ch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random subset of data\n",
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Wide_ResNet\n",
    "# Uses The Google Research Authors, file wideresnet.py\n",
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm, dropout_rate=dropout_rate)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, args.n_classes)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energies if y=none\n",
    "# EBM energy calculated as logsumexp of logits\n",
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm, dropout_rate=dropout_rate, n_classes=n_classes)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            # gathers the logits along dim 1 with indeces y\n",
    "            return t.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various utilities\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def init_random(args, bs):\n",
    "    return t.FloatTensor(bs, n_ch, im_sz, im_sz).uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected line from the function of the same name in train_wrn_ebm.py\n",
    "def get_model_and_buffer(args, device):\n",
    "    model_cls = F if args.uncond else CCF\n",
    "    f = model_cls(args.depth, args.width, args.norm)\n",
    "    \n",
    "    print(f\"loading model from {args.load_path}\")\n",
    "    ckpt_dict = t.load(args.load_path)\n",
    "    f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "    try:\n",
    "        replay_buffer = ckpt_dict[\"replay_buffer\"]\n",
    "    except:\n",
    "        replay_buffer = None\n",
    "    try:\n",
    "        epoch = ckpt_dict[\"epoch\"]\n",
    "    except:\n",
    "        epoch=0\n",
    "    \n",
    "    f = f.to(device)\n",
    "    return f, replay_buffer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in chosen dataset from svhn, cifar10, cifar100\n",
    "def get_data(args):\n",
    "    if args.dataset == \"svhn\":\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "    else:\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.RandomHorizontalFlip(),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),]\n",
    "        )\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5))]\n",
    "         #lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "    def dataset_fn(train, transform):\n",
    "        if args.dataset == \"cifar10\":\n",
    "            return tv.datasets.CIFAR10(root=args.data_root, transform=transform, download=True, train=train)\n",
    "        elif args.dataset == \"cifar100\":\n",
    "            return tv.datasets.CIFAR100(root=args.data_root, transform=transform, download=True, train=train)\n",
    "        else:\n",
    "            return tv.datasets.SVHN(root=args.data_root, transform=transform, download=True,\n",
    "                                    split=\"train\" if train else \"test\")\n",
    "\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    #Set up index variables\n",
    "    full_train = dataset_fn(True, transform_train)\n",
    "    all_inds = list(range(len(full_train)))\n",
    "    train_inds = np.array(all_inds)\n",
    "    train_labeled_inds = []\n",
    "    other_inds = []\n",
    "    #print(type(full_train),\"\\n-----------------------------------------------\\n\",train_inds)\n",
    "    train_labels = np.array([full_train[ind][1] for ind in train_inds])\n",
    "    \n",
    "    #Assign indexes b/w train and train_labeled (Default: all are train)\n",
    "    if args.labels_per_class > 0:\n",
    "        for i in range(args.n_classes):\n",
    "            print(i)\n",
    "            train_labeled_inds.extend(train_inds[train_labels == i][:args.labels_per_class])\n",
    "            other_inds.extend(train_inds[train_labels == i][args.labels_per_class:])\n",
    "    else:\n",
    "        train_labeled_inds = train_inds\n",
    "\n",
    "    dset_train = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_inds)\n",
    "    \n",
    "    dset_train_labeled = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_labeled_inds)\n",
    "    \n",
    "    #Convert to DataLoaders\n",
    "    dload_train = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = DataLoader(dset_train_labeled, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = cycle(dload_train_labeled)\n",
    "    dset_test = dataset_fn(False, transform_test)\n",
    "    dload_test = DataLoader(dset_test, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    \n",
    "    return dload_train, dload_train_labeled, dload_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid repeat code and maintanence. This is for the evaluations\n",
    "def eval_classification_inner(f,dload,device):\n",
    "    softmax=nn.Softmax(dim=1)\n",
    "    corrects, losses, logits_all = [], [], []\n",
    "    for x_p_d, y_p_d in dload:\n",
    "        x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "        logits = f.classify(x_p_d)\n",
    "        logits_all.extend(logits)\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().numpy()\n",
    "        losses.extend(loss)\n",
    "        \n",
    "        correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "        corrects.extend(correct)\n",
    "    logits_all=t.stack(logits_all)\n",
    "    logits=softmax(logits_all)\n",
    "    sms = logits.max(1)[0]\n",
    "    cali_vals=[(a,b.item()) for a,b in zip(corrects,sms)]\n",
    "    return corrects, losses, cali_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss and accuracy for periodic printout\n",
    "def eval_classification(f, dload, device):\n",
    "    corrects, losses, _ = eval_classification_inner(f,dload,device)\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    return correct, loss, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate loss and accuracy for calibration\n",
    "def eval_with_calibration(f, dload, device):\n",
    "    corrects, losses, cali_vals = eval_classification_inner(f,dload,device)\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    return correct, loss, cali_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the calibration data to a file\n",
    "def save_calibration(filename,cali_vals):\n",
    "    with open(filename,\"w\") as f:\n",
    "        f.write(\"correct,softmax\\n\")\n",
    "        for i in cali_vals:\n",
    "            f.write(\"{},{}\\n\".format(i[0],i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track loss for convergence\n",
    "def loss_tracker(filename,epoch,loss,correct):\n",
    "    if not os.path.isfile(os.path.join(args.save_dir,filename)):\n",
    "        with open(os.path.join(args.save_dir,filename),'w') as of:\n",
    "            of.write(\"Epoch,Loss,Acc\\n\")\n",
    "            of.write(\"{},{},{}\\n\".format(epoch,loss,correct))\n",
    "    else:\n",
    "        with open(os.path.join(args.save_dir,filename),'a') as of:\n",
    "            of.write(\"{},{},{}\\n\".format(epoch,loss,correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for training\n",
    "# Uses args from class below\n",
    "def main(args):\n",
    "    ######################################################\n",
    "    ###                                                ###\n",
    "    ###               Closure functions                ###\n",
    "    ###                                                ###\n",
    "    ######################################################\n",
    "    \n",
    "    #Does eval and saving for the test and training datasets\n",
    "    def eval_both(eval_type,with_tracker=False):\n",
    "        evs=['test', 'train']\n",
    "        dls=[dload_test,dload_train]\n",
    "        for ev,dl in zip(evs,dls):\n",
    "            correct, loss, cv = eval_type(f, dl, device)\n",
    "            if eval_type==eval_with_calibration: \n",
    "                save_calibration(os.path.join(args.save_dir,f'cali_{ev}.csv'),cv)\n",
    "            if with_tracker:\n",
    "                loss_tracker(f'track_{ev}.csv',epoch,loss,correct)\n",
    "            print(f\"{ev}: Epoch {epoch}: Valid Loss {loss}, Valid Acc {correct}\")\n",
    "            \n",
    "    ######################################################\n",
    "    ###                                                ###\n",
    "    ###                  Start main                    ###\n",
    "    ###                                                ###\n",
    "    ######################################################\n",
    "            \n",
    "    utils.makedirs(args.save_dir)\n",
    "\n",
    "    if args.print_to_log:\n",
    "        sys.stdout = open(f'{args.save_dir}/log.txt', 'w')\n",
    "\n",
    "    t.manual_seed(seed)\n",
    "    if t.cuda.is_available():\n",
    "        t.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # datasets - note this evaluation has no validation set\n",
    "    dload_train, dload_train_labeled, dload_test = get_data(args)\n",
    "    \n",
    "    # select device\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    #MODEL\n",
    "    f, buffer, epoch = get_model_and_buffer(args, device)\n",
    "    \n",
    "\n",
    "    \n",
    "    #Evaluation - currently set to evaluate only the train and test sets.\n",
    "    f.eval()\n",
    "    with t.no_grad():\n",
    "        eval_both(eval_with_calibration,with_tracker=True,)\n",
    "    f.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "# defaults for paper\n",
    "# --lr .0001 --dataset cifar10 --optimizer adam --p_x_weight 1.0 --p_y_given_x_weight 1.0 \n",
    "# --p_x_y_weight 0.0 --sigma .03 --width 10 --depth 28 --save_dir /YOUR/SAVE/DIR \n",
    "# --plot_uncond --warmup_iters 1000\n",
    "#\n",
    "# Regression\n",
    "# {\"dataset\": \"cifar10\", \"data_root\": \"../data\", \"lr\": 0.0001, \"decay_epochs\": [160, 180], \n",
    "# \"decay_rate\": 0.3, \"clf_only\": false, \"labels_per_class\": -1, \"optimizer\": \"adam\", \n",
    "# \"batch_size\": 64, \"n_epochs\": 200, \"warmup_iters\": 1000, \"p_x_weight\": 1.0, \n",
    "# \"p_y_given_x_weight\": 1.0, \"p_x_y_weight\": 0.0, \"dropout_rate\": 0.0, \"sigma\": 0.03, \n",
    "# \"weight_decay\": 0.0, \"norm\": null, \"n_steps\": 20, \"width\": 10, \"depth\": 28, \"uncond\": false, \n",
    "# \"class_cond_p_x_sample\": false, \"buffer_size\": 10000, \"reinit_freq\": 0.05, \"sgld_lr\": 1.0, \n",
    "# \"sgld_std\": 0.01, \"save_dir\": \"./savedir\", \"ckpt_every\": 10, \"eval_every\": 1, \n",
    "# \"print_every\": 100, \"load_path\": null, \"print_to_log\": false, \"plot_cond\": false, \n",
    "#\"plot_uncond\": true, \"n_valid\": 5000, \"n_classes\": 10}\n",
    "class train_args():\n",
    "    def __init__(self, param_dict):\n",
    "        # set defaults\n",
    "        self.dataset = \"cifar10\" #, choices=[\"cifar10\", \"svhn\", \"cifar100\"])\n",
    "        self.n_classes = 100 if self.dataset == \"cifar100\" else 10\n",
    "        self.data_root = \"../data\" \n",
    "        # optimization\n",
    "        self.lr = 1e-4\n",
    "        self.decay_epochs = [160, 180] # help=\"decay learning rate by decay_rate at these epochs\")\n",
    "        self.decay_rate = .3 # help=\"learning rate decay multiplier\")\n",
    "        self.clf_only = False #action=\"store_true\", help=\"If set, then only train the classifier\")\n",
    "        self.labels_per_class = -1# help=\"number of labeled examples per class, if zero then use all labels\")\n",
    "        self.optimizer = \"adam\" #choices=[\"adam\", \"sgd\"], default=\"adam\")\n",
    "        self.batch_size = 64\n",
    "        self.n_epochs = 200\n",
    "        self.warmup_iters = -1 # help=\"number of iters to linearly increase learning rate, if -1 then no warmmup\")\n",
    "        # loss weighting\n",
    "        self.p_x_weight = 1.\n",
    "        self.p_y_given_x_weight = 1.\n",
    "        self.p_x_y_weight = 0.\n",
    "        # regularization\n",
    "        self.dropout_rate = 0.0\n",
    "        self.sigma = 3e-2 # help=\"stddev of gaussian noise to add to input, .03 works but .1 is more stable\")\n",
    "        self.weight_decay = 0.0\n",
    "        # network\n",
    "        self.norm = None # choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"], help=\"norm to add to weights, none works fine\")\n",
    "        # EBM specific\n",
    "        self.n_steps = 20 # help=\"number of steps of SGLD per iteration, 100 works for short-run, 20 works for PCD\")\n",
    "        self.width = 10 # help=\"WRN width parameter\")\n",
    "        self.depth = 28 # help=\"WRN depth parameter\")\n",
    "        self.uncond = False # \"store_true\" # help=\"If set, then the EBM is unconditional\")\n",
    "        self.class_cond_p_x_sample = False #, action=\"store_true\", help=\"If set we sample from p(y)p(x|y), othewise sample from p(x),\" \"Sample quality higher if set, but classification accuracy better if not.\")\n",
    "        self.buffer_size = 10000\n",
    "        self.reinit_freq = .05\n",
    "        self.sgld_lr = 1.0\n",
    "        self.sgld_std = 1e-2\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.ckpt_every = 10 # help=\"Epochs between checkpoint save\")\n",
    "        self.eval_every = 1 # help=\"Epochs between evaluation\")\n",
    "        self.print_every = 100 # help=\"Iterations between print\")\n",
    "        self.load_path = None # path for checkpoint to load\n",
    "        self.print_to_log = False #\", action=\"store_true\", help=\"If true, directs std-out to log file\")\n",
    "        self.plot_cond = False #\", action=\"store_true\", help=\"If set, save class-conditional samples\")\n",
    "        self.plot_uncond = False #\", action=\"store_true\", help=\"If set, save unconditional samples\")\n",
    "        self.n_valid = 5000\n",
    "        self.exp_reason = \"\"\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loading model...\n",
      "| Wide-Resnet 28x10\n",
      "loading model from ./train5/best_valid_ckpt.pt\n",
      "test: Epoch 8: Valid Loss 0.8682469725608826, Valid Acc 0.6976000070571899\n",
      "train: Epoch 8: Valid Loss 0.8316287994384766, Valid Acc 0.7070462703704834\n"
     ]
    }
   ],
   "source": [
    "#Set up changes to default params\n",
    "inline_parms = {\"dataset\": \"cifar100\", \"save_dir\": './production/theirs5', \\\n",
    "                \"sigma\": .03, \"width\": 10, \"depth\": 28, \"plot_uncond\": False, \\\n",
    "                \"uncond\": False, \\\n",
    "                \"load_path\": './train5/best_valid_ckpt.pt'}\n",
    "\n",
    "# instantiate\n",
    "args = train_args(inline_parms)\n",
    "\n",
    "# run\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./production_cali_paper/ours/run1\n",
      "./production_cali_paper/ours/run2\n",
      "./production_cali_paper/ours/run3\n",
      "./production_cali_paper/ours/run4\n",
      "./production_cali_paper/ours/run5\n",
      "./production_cali_paper/ours/run6\n",
      "./production_cali_paper/ours/run7\n",
      "./production_cali_paper/ours/run8\n",
      "./production_cali_paper/ours/run9\n",
      "./production_cali_paper/ours/run10\n",
      "./production_cali_paper/ours/run11\n",
      "./production_cali_paper/ours/run12\n",
      "./production_cali_paper/theirs/run1\n",
      "./production_cali_paper/theirs/run2\n",
      "./production_cali_paper/theirs/run3\n",
      "./production_cali_paper/theirs/run4\n",
      "./production_cali_paper/theirs/run5\n",
      "./production_cali_paper/theirs/run6\n",
      "./production_cali_paper/theirs/run7\n",
      "./production_cali_paper/theirs/run8\n",
      "./production_cali_paper/theirs/run9\n",
      "./production_cali_paper/theirs/run10\n",
      "./production_cali_paper/theirs/run11\n",
      "./production_cali_paper/theirs/run12\n",
      "./production_cali_paper/baseline/run1\n",
      "./production_cali_paper/baseline/run2\n",
      "./production_cali_paper/baseline/run3\n",
      "./production_cali_paper/baseline/run4\n",
      "./production_cali_paper/baseline/run5\n",
      "./production_cali_paper/baseline/run6\n",
      "./production_cali_paper/baseline/run7\n",
      "./production_cali_paper/baseline/run8\n",
      "./production_cali_paper/baseline/run9\n",
      "./production_cali_paper/baseline/run10\n",
      "./production_cali_paper/baseline/run11\n",
      "./production_cali_paper/baseline/run12\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loading model...\n",
      "| Wide-Resnet 28x10\n",
      "loading model from ./production_cali_cf100/baseline/run11/best_valid_ckpt.pt\n",
      "test: Epoch 186: Valid Loss 2.924657106399536, Valid Acc 0.7214000225067139\n",
      "train: Epoch 186: Valid Loss 0.2882467806339264, Valid Acc 0.9722111225128174\n",
      "./production_cali_cf100/baseline/run11\n"
     ]
    }
   ],
   "source": [
    "#Loopable\n",
    "base='./production_cali_cf100/'\n",
    "expdirs=[i for i in os.listdir(base) if os.path.isdir(os.path.join(base,i))]\n",
    "expdirs=['baseline']\n",
    "for exp in expdirs:\n",
    "    for i in range(11,12):\n",
    "        if exp=='theirs' or exp=='baseline':\n",
    "            run = os.path.join(base,exp,\"run\"+str(i))\n",
    "        else:\n",
    "            run = os.path.join(base,exp,\"cvrlrun\"+str(i))\n",
    "\n",
    "        #Set up changes to default params\n",
    "        inline_parms = {\"dataset\": \"cifar100\", \"save_dir\": run, \\\n",
    "                        \"sigma\": .03, \"width\": 10, \"depth\": 28, \"plot_uncond\": False, \\\n",
    "                        \"uncond\": False, \\\n",
    "                        \"load_path\": os.path.join(run,'best_valid_ckpt.pt')}\n",
    "\n",
    "        # instantiate\n",
    "        args = train_args(inline_parms)\n",
    "        args.n_classes = 100 if args.dataset == \"cifar100\" else 10\n",
    "        \n",
    "        # run\n",
    "        main(args)\n",
    "        print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
