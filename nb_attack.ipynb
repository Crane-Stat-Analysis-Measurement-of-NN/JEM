{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import numpy as np\n",
    "import pdb\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import foolbox\n",
    "import wideresnet\n",
    "from collections import OrderedDict\n",
    "from utils import *\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_module_state_dict(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key in state_dict.keys():\n",
    "        new_key = '.'.join(key.split('.')[1:])\n",
    "        new_state_dict[new_key] = state_dict[key]\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_args():\n",
    "    def __init__(self, param_dict):\n",
    "        # training\n",
    "        self.dataset = 'cifar'\n",
    "        self.batch_size = 50\n",
    "        self.norm = None\n",
    "\n",
    "        # EBM specific\n",
    "        self.n_steps = 100 \n",
    "        self.width = 10\n",
    "        self.depth = 28\n",
    "        # \n",
    "        self.n_steps_refine = 0\n",
    "        self.n_classes = 10\n",
    "        self.init_batch_size = 128\n",
    "        self.softmax_ce = True\n",
    "        # attack\n",
    "        self.attack_conf = True\n",
    "        self.random_init= True\n",
    "        self.threshold = .7\n",
    "        self.debug = True\n",
    "        self.no_random_start = True\n",
    "        self.load_path = None\n",
    "        self.distance = 'Linf'\n",
    "        self.n_steps_pgd_attack = 40\n",
    "        self.start_batch = -1\n",
    "        self.end_batch = 2\n",
    "        self.sgld_sigma = 1e-2\n",
    "        self.n_dup_chains = 5\n",
    "        self.sigma = .03\n",
    "        self.base_dir = './adv_results'\n",
    "        # logging\n",
    "        self.exp_name = 'exp'\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup change from defaults\n",
    "inline_parms = {\"load_path\": \"./production/ours_nb/run1/best_valid_ckpt.pt\"}\n",
    "\n",
    "# instantiate\n",
    "args = train_args(inline_parms)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "#args_ = vars(args)\n",
    "#for key in args_.keys():\n",
    "#    print('{}:   {}'.format(key,args_[key]))\n",
    "base_dir = args.base_dir\n",
    "save_dir = os.path.join(base_dir, args.exp_name, 'saved_model')\n",
    "last_dir = os.path.join(save_dir,'last')\n",
    "best_dir = os.path.join(save_dir,'best')\n",
    "data_dir = os.path.join(base_dir,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gradient_attack_wrapper(nn.Module):\n",
    "  def __init__(self, model):\n",
    "    super(gradient_attack_wrapper, self).__init__()\n",
    "    self.model = model.eval()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x - 0.5\n",
    "    x = x / 0.5\n",
    "    x.requires_grad_()\n",
    "    out = self.model.module.refined_logits(x)\n",
    "    return out\n",
    "\n",
    "  def eval(self):\n",
    "    return self.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "model_attack_wrapper =gradient_attack_wrapper\n",
    "\n",
    "transformer_train  = transforms.Compose([transforms.ToTensor()])\n",
    "transformer_test  = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data_loader  = torch.utils.data.DataLoader(datasets.CIFAR10(data_dir, train=False,\n",
    "                                                            transform=transformer_test, download=True),\n",
    "                                           batch_size=args.batch_size, shuffle=False, num_workers=10)\n",
    "init_loader = torch.utils.data.DataLoader(datasets.CIFAR10(data_dir, train=True,\n",
    "                                                           download=True, transform=transformer_train),\n",
    "                                          batch_size=args.init_batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, 10)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z)\n",
    "\n",
    "\n",
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            return torch.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Wide-Resnet 28x10\n",
      "ckpt_48_1-23.pt\n",
      "loading model from {args.load_path}\n"
     ]
    }
   ],
   "source": [
    "# construct model and ship to GPU\n",
    "f = CCF(args.depth, args.width, args.norm)\n",
    "print(args.load_path)\n",
    "print(\"loading model from {args.load_path}\")\n",
    "ckpt_dict = torch.load(args.load_path)\n",
    "if \"model_state_dict\" in ckpt_dict:\n",
    "    # loading from a new checkpoint\n",
    "    f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "else:\n",
    "    # loading from an old checkpoint\n",
    "    f.load_state_dict(ckpt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper class to provide utilities for what you need\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super(DummyModel, self).__init__()\n",
    "        self.f = f\n",
    "\n",
    "    def logits(self, x):\n",
    "        return self.f.classify(x)\n",
    "\n",
    "    def refined_logits(self, x, n_steps=args.n_steps_refine):\n",
    "        xs = x.size()\n",
    "        dup_x = x.view(xs[0], 1, xs[1], xs[2], xs[3]).repeat(1, args.n_dup_chains, 1, 1, 1)\n",
    "        dup_x = dup_x.view(xs[0] * args.n_dup_chains, xs[1], xs[2], xs[3])\n",
    "        dup_x = dup_x + torch.randn_like(dup_x) * args.sigma\n",
    "        refined = self.refine(dup_x, n_steps=n_steps, detach=False)\n",
    "        logits = self.logits(refined)\n",
    "        logits = logits.view(x.size(0), args.n_dup_chains, logits.size(1))\n",
    "        logits = logits.mean(1)\n",
    "        return logits\n",
    "\n",
    "    def classify(self, x):\n",
    "        logits = self.logits(x)\n",
    "        pred = logits.max(1)[1]\n",
    "        return pred\n",
    "\n",
    "    def logpx_score(self, x):\n",
    "        # unnormalized logprob, unconditional on class\n",
    "        return self.f(x)\n",
    "\n",
    "    def refine(self, x, n_steps=args.n_steps_refine, detach=True):\n",
    "        # runs a markov chain seeded at x, use n_steps=10\n",
    "        x_k = torch.autograd.Variable(x, requires_grad=True) if detach else x\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            f_prime = torch.autograd.grad(self.f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "            x_k.data += f_prime + args.sgld_sigma * torch.randn_like(x_k)\n",
    "        final_samples = x_k.detach() if detach else x_k\n",
    "        return final_samples\n",
    "\n",
    "    def grad_norm(self, x):\n",
    "        x_k = torch.autograd.Variable(x, requires_grad=True)\n",
    "        f_prime = torch.autograd.grad(self.f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "        grad = f_prime.view(x.size(0), -1)\n",
    "        return grad.norm(p=2, dim=1)\n",
    "\n",
    "    def logpx_delta_score(self, x, n_steps=args.n_steps_refine):\n",
    "        # difference in logprobs from input x and samples from a markov chain seeded at x\n",
    "        #\n",
    "        init_scores = self.f(x)\n",
    "        x_r = self.refine(x, n_steps=n_steps)\n",
    "        final_scores = self.f(x_r)\n",
    "        # for real data final_score is only slightly higher than init_score\n",
    "        return init_scores - final_scores\n",
    "\n",
    "    def logp_grad_score(self, x):\n",
    "        return -self.grad_norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch365/jpiland/.local/lib/python3.6/site-packages/foolbox/models/pytorch.py:62: UserWarning: The PyTorch model is in training mode and therefore might not be deterministic. Call the eval() method to set it in evaluation mode if this is not intended.\n",
      "  'The PyTorch model is in training mode and therefore might'\n",
      "/scratch365/jpiland/.local/lib/python3.6/site-packages/foolbox/attacks/base.py:129: UserWarning: Not running the attack because the original input is already misclassified and the adversarial thus has a distance of 0.\n",
      "  warnings.warn('Not running the attack because the original input'\n"
     ]
    }
   ],
   "source": [
    "f = DummyModel(f)\n",
    "model = f.to(device)\n",
    "model = nn.DataParallel(model).to(device)\n",
    "\n",
    "model.eval()\n",
    "## Define criterion\n",
    "criterion = foolbox.criteria.Misclassification()\n",
    "\n",
    "## Initiate attack and wrap model\n",
    "model_wrapped = model_attack_wrapper(model)\n",
    "fmodel = foolbox.models.PyTorchModel(model_wrapped, bounds=(0.,1.), num_classes=10, device=device)\n",
    "\n",
    "if args.distance == 'L2':\n",
    "    distance = foolbox.distances.MeanSquaredDistance\n",
    "    attack = foolbox.attacks.L2BasicIterativeAttack(model=fmodel, criterion=criterion, distance=distance)\n",
    "else:\n",
    "    distance = foolbox.distances.Linfinity\n",
    "    attack = foolbox.attacks.RandomStartProjectedGradientDescentAttack(model=fmodel, criterion=criterion, distance=distance)\n",
    "\n",
    "print('Starting...')\n",
    "for i, (img, label) in enumerate(data_loader):\n",
    "    adversaries = []\n",
    "    if i < args.start_batch:\n",
    "        continue\n",
    "    if i >= args.end_batch:\n",
    "      break\n",
    "    img = img.data.cpu().numpy()\n",
    "    logits = model_wrapped(torch.from_numpy(img[:, :, :, :]).to(device))\n",
    "    _, top = torch.topk(logits,k=2,dim=1)\n",
    "    top = top.data.cpu().numpy()\n",
    "    pred = top[:,0]\n",
    "    for k in range(len(label)):\n",
    "      im = img[k,:,:,:]\n",
    "      orig_label = label[k].data.cpu().numpy()\n",
    "      if pred[k] != orig_label:\n",
    "        continue\n",
    "      best_adv = None\n",
    "      for ii in range(20):\n",
    "          try:\n",
    "            adversarial = attack(im, label=orig_label, unpack=False, random_start=True, iterations=args.n_steps_pgd_attack) \n",
    "            if ii == 0 or best_adv.distance > adversarial.distance:\n",
    "                best_adv = adversarial\n",
    "          except:\n",
    "            continue\n",
    "      try:\n",
    "          adversaries.append((im, orig_label, adversarial.image, adversarial.adversarial_class))\n",
    "      except:\n",
    "          continue\n",
    "    adv_save_dir = os.path.join(base_dir, args.exp_name)\n",
    "    save_file = 'adversarials_batch_'+str(i)\n",
    "    if not os.path.exists(os.path.join(adv_save_dir,save_file)):\n",
    "        os.makedirs(os.path.join(adv_save_dir,save_file))\n",
    "    np.save(os.path.join(adv_save_dir,save_file),adversaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
