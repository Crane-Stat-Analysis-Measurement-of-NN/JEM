{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import numpy as np\n",
    "import pdb\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import foolbox\n",
    "import wideresnet\n",
    "from collections import OrderedDict\n",
    "from utils import *\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "def remove_module_state_dict(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key in state_dict.keys():\n",
    "        new_key = '.'.join(key.split('.')[1:])\n",
    "        new_state_dict[new_key] = state_dict[key]\n",
    "    return new_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "class eval_args():\n",
    "    def __init__(self, param_dict):\n",
    "        # training\n",
    "        self.dataset = 'cifar' #parser.add_argument('--dataset', type=str, default='cifar')\n",
    "        self.batch_size = 50 #parser.add_argument('--batch_size', type=int, default=50)\n",
    "        self.norm = None #parser.add_argument(\"--norm\", type=str, default=None, choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"])\n",
    "\n",
    "        # EBM specific\n",
    "        self.n_steps = 100 #parser.add_argument(\"--n_steps\", type=int, default=100)\n",
    "        self.width = 10 #parser.add_argument(\"--width\", type=int, default=10)\n",
    "        self.depth = 28 #parser.add_argument(\"--depth\", type=int, default=28)\n",
    "        # \n",
    "        self.n_steps_refine = 0 #parser.add_argument('--n_steps_refine', type=int, default=0)\n",
    "        self.n_classes = 10 #parser.add_argument('--n_classes',type=int,default=10)\n",
    "        self.init_batch_size = 128 #parser.add_argument('--init_batch_size', type=int, default=128)\n",
    "        self.softmax_ce = False #parser.add_argument('--softmax_ce', action='store_true')\n",
    "        # attack\n",
    "        self.attack_conf = False #parser.add_argument('--attack_conf',  action='store_true')\n",
    "        self.random_init = False #parser.add_argument('--random_init',  action='store_true')\n",
    "        self.threshold = .7 #parser.add_argument('--threshold', type=float, default=.7)\n",
    "        self.debug = False #parser.add_argument('--debug',  action='store_true')\n",
    "        self.no_random_start = False #parser.add_argument('--no_random_start',  action='store_true')\n",
    "        self.load_path = None #parser.add_argument(\"--load_path\", type=str, default=None)\n",
    "        self.distance = 'Linf' #parser.add_argument(\"--distance\", type=str, default='Linf')\n",
    "        self.n_steps_pgd_attack = 40 #parser.add_argument(\"--n_steps_pgd_attack\", type=int, default=40)\n",
    "        self.start_batch = -1 #parser.add_argument(\"--start_batch\", type=int, default=-1)\n",
    "        self.end_batch = 2 #parser.add_argument(\"--end_batch\", type=int, default=2)\n",
    "        self.sgld_sigma = 1e-2 #parser.add_argument(\"--sgld_sigma\", type=float, default=1e-2)\n",
    "        self.n_dup_chains = 5 #parser.add_argument(\"--n_dup_chains\", type=int, default=5)\n",
    "        self.sigma = .03 #parser.add_argument(\"--sigma\", type=float, default=.03)\n",
    "        self.base_dir = './experiment_attacks' #parser.add_argument(\"--base_dir\", type=str, default='./adv_results')\n",
    "        # added\n",
    "        self.exp_name = 'exp' #parser.add_argument('--exp_name', type=str, default='exp', help='saves everything in ?r/exp_name/')\n",
    "\n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "# --start_batch 0 --end_batch 6 --load_path /cloud_storage/BEST_EBM.pt \n",
    "#--exp_name rerun_ebm_1_step_5_dup_l2_no_sigma_REDO --n_steps_refine 1 --distance L2 \n",
    "#--random_init --n_dup_chains 5 --sigma 0.0 --base_dir /cloud_storage/adv_results &\n",
    "args = eval_args({\"load_path\": \"./experiment_weights/energy_with_aug_L2_dlogpx_dx_3/last_ckpt.pt\", \"distance\": 'L2', \\\n",
    "                 \"start_batch\": 0, \"end_batch\": 1, \"n_dup_chains\": 5, \"sigma\": 0.0, \"random_init\": True, \\\n",
    "                 \"batch_size\": 50})\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# locations\n",
    "\n",
    "base_dir = args.base_dir\n",
    "\n",
    "save_dir = os.path.join(base_dir, args.exp_name, 'saved_model')\n",
    "last_dir = os.path.join(save_dir,'last')\n",
    "best_dir = os.path.join(save_dir,'best')\n",
    "data_dir = os.path.join(base_dir,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gradient_attack_wrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(gradient_attack_wrapper, self).__init__()\n",
    "        self.model = model.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x - 0.5\n",
    "        x = x / 0.5\n",
    "        x.requires_grad_()\n",
    "        out = self.model.module.refined_logits(x)\n",
    "        return out\n",
    "\n",
    "    def eval(self):\n",
    "        return self.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, 10)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            return torch.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper class to provide utilities for what you need\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super(DummyModel, self).__init__()\n",
    "        self.f = f\n",
    "\n",
    "    def logits(self, x):\n",
    "        return self.f.classify(x)\n",
    "\n",
    "    def refined_logits(self, x, n_steps=args.n_steps_refine):\n",
    "        xs = x.size()\n",
    "        dup_x = x.view(xs[0], 1, xs[1], xs[2], xs[3]).repeat(1, args.n_dup_chains, 1, 1, 1)\n",
    "        dup_x = dup_x.view(xs[0] * args.n_dup_chains, xs[1], xs[2], xs[3])\n",
    "        dup_x = dup_x + torch.randn_like(dup_x) * args.sigma\n",
    "        refined = self.refine(dup_x, n_steps=n_steps, detach=False)\n",
    "        logits = self.logits(refined)\n",
    "        logits = logits.view(x.size(0), args.n_dup_chains, logits.size(1))\n",
    "        logits = logits.mean(1)\n",
    "        return logits\n",
    "\n",
    "    def classify(self, x):\n",
    "        logits = self.logits(x)\n",
    "        pred = logits.max(1)[1]\n",
    "        return pred\n",
    "\n",
    "    def logpx_score(self, x):\n",
    "        # unnormalized logprob, unconditional on class\n",
    "        return self.f(x)\n",
    "\n",
    "    def refine(self, x, n_steps=args.n_steps_refine, detach=True):\n",
    "        # runs a markov chain seeded at x, use n_steps=10\n",
    "        x_k = torch.autograd.Variable(x, requires_grad=True) if detach else x\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            f_prime = torch.autograd.grad(self.f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "            x_k.data += f_prime + args.sgld_sigma * torch.randn_like(x_k)\n",
    "        final_samples = x_k.detach() if detach else x_k\n",
    "        return final_samples\n",
    "\n",
    "    def grad_norm(self, x):\n",
    "        x_k = torch.autograd.Variable(x, requires_grad=True)\n",
    "        f_prime = torch.autograd.grad(self.f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "        grad = f_prime.view(x.size(0), -1)\n",
    "        return grad.norm(p=2, dim=1)\n",
    "\n",
    "    def logpx_delta_score(self, x, n_steps=args.n_steps_refine):\n",
    "        # difference in logprobs from input x and samples from a markov chain seeded at x\n",
    "        #\n",
    "        init_scores = self.f(x)\n",
    "        x_r = self.refine(x, n_steps=n_steps)\n",
    "        final_scores = self.f(x_r)\n",
    "        # for real data final_score is only slightly higher than init_score\n",
    "        return init_scores - final_scores\n",
    "\n",
    "    def logp_grad_score(self, x):\n",
    "        return -self.grad_norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attack_wrapper =gradient_attack_wrapper\n",
    "\n",
    "transformer_train  = transforms.Compose([transforms.ToTensor()])\n",
    "transformer_test  = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data_loader  = torch.utils.data.DataLoader(datasets.CIFAR10(data_dir, train=False,\n",
    "                                                            transform=transformer_test, download=True),\n",
    "                                           batch_size=args.batch_size, shuffle=False, num_workers=10)\n",
    "init_loader = torch.utils.data.DataLoader(datasets.CIFAR10(data_dir, train=True,\n",
    "                                                           download=True, transform=transformer_train),\n",
    "                                          batch_size=args.init_batch_size, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model and ship to GPU\n",
    "f = CCF(args.depth, args.width, args.norm)\n",
    "print(args.load_path)\n",
    "print(\"loading model from {args.load_path}\")\n",
    "ckpt_dict = torch.load(args.load_path)\n",
    "if \"model_state_dict\" in ckpt_dict:\n",
    "    # loading from a new checkpoint\n",
    "    f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "else:\n",
    "    # loading from an old checkpoint\n",
    "    f.load_state_dict(ckpt_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DummyModel(f)\n",
    "model = f.to(device)\n",
    "model = nn.DataParallel(model).to(device)\n",
    "\n",
    "model.eval()\n",
    "## Define criterion\n",
    "criterion = foolbox.criteria.Misclassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiate attack and wrap model\n",
    "model_wrapped = model_attack_wrapper(model)\n",
    "fmodel = foolbox.models.PyTorchModel(model_wrapped, bounds=(0.,1.), num_classes=10, device=device)\n",
    "\n",
    "if args.distance == 'L2':\n",
    "    distance = foolbox.distances.MeanSquaredDistance\n",
    "    attack = foolbox.attacks.L2BasicIterativeAttack(model=fmodel, criterion=criterion, distance=distance)\n",
    "else:\n",
    "    distance = foolbox.distances.Linfinity\n",
    "    attack = foolbox.attacks.RandomStartProjectedGradientDescentAttack(model=fmodel, criterion=criterion, distance=distance)\n",
    "\n",
    "print('Starting...')\n",
    "# Grab 50 images at a time (k index), go though batches start_batch to end_batch (i index)\n",
    "for i, (img, label) in enumerate(data_loader):\n",
    "    print(\"i... \", i)\n",
    "    adversaries = []\n",
    "    if i < args.start_batch:\n",
    "        continue\n",
    "    if i >= args.end_batch:\n",
    "        break\n",
    "    # get images and their logits from classification with the model\n",
    "    img = img.data.cpu().numpy()\n",
    "    logits = model_wrapped(torch.from_numpy(img[:, :, :, :]).to(device))\n",
    "    \n",
    "    # Get the k largest elements of the logits along dimention 1\n",
    "    _, top = torch.topk(logits,k=2,dim=1)\n",
    "    top = top.data.cpu().numpy()\n",
    "    # prediction is the first element\n",
    "    pred = top[:,0]\n",
    "    \n",
    "    # loop through the 50 images\n",
    "    #print(\"pred\",pred,len(label))\n",
    "    for k in range(len(label)):\n",
    "        print(\"k... \", k)\n",
    "        \n",
    "        # get image and its label\n",
    "        im = img[k,:,:,:]\n",
    "        orig_label = label[k].data.cpu().numpy()\n",
    "        \n",
    "        # check its croorectly classified else we cant use for mis-classification\n",
    "        if pred[k] != orig_label:\n",
    "            print(\"Error in prediction\")\n",
    "            continue\n",
    "            \n",
    "        # setup for adversarial with lowest distance\n",
    "        best_adv = None\n",
    "        \n",
    "        # try 20 times for best\n",
    "        for ii in range(1): #20):\n",
    "            try:\n",
    "                # create adversarial\n",
    "                adversarial = attack(im, label=orig_label, unpack=False, random_start=True, iterations=args.n_steps_pgd_attack) \n",
    "                print(\"adv\", adversarial.distance)\n",
    "                \n",
    "                # save if its the lowest distance\n",
    "                # I guess it iterates to failure and we want the smallest of these\n",
    "                if ii == 0 or best_adv.distance > adversarial.distance:\n",
    "                    best_adv = adversarial\n",
    "            except Exception as e: \n",
    "                print(\"Failed Adv.\", e)\n",
    "                continue\n",
    "        try:\n",
    "            # save our adversarial\n",
    "            adversaries.append((im, orig_label, adversarial.image, adversarial.adversarial_class))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # save our adversarials\n",
    "    adv_save_dir = os.path.join(base_dir, args.exp_name)\n",
    "    save_file = 'adversarials_batch_'+str(i)\n",
    "    if not os.path.exists(os.path.join(adv_save_dir,save_file)):\n",
    "        os.makedirs(os.path.join(adv_save_dir,save_file))\n",
    "    np.save(os.path.join(adv_save_dir,save_file),adversaries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load our 7 images\n",
    "#np.save('./experiment_attacks/exp/adversarials_batch_test2.npy', adversaries)\n",
    "#adversaries_rl = np.load(\"./experiment_attacks/exp/adversarials_batch_test2.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "def find_epsilons(adversaries_rl):\n",
    "    adv_size = adversaries_rl.shape[0]\n",
    "    eps = np.zeros(adv_size + 1)\n",
    "    acc = np.zeros(adv_size + 1)\n",
    "    eps[0] = 0.\n",
    "    acc[0] = 1.\n",
    "\n",
    "    for i in range(adv_size):\n",
    "        perturbation = adversaries_rl[i,2] - adversaries_rl[i,0]\n",
    "        #print('L2 norm of perturbation: {}'.format(np.linalg.norm(perturbation.flatten()*255, 2)))\n",
    "        eps[i+1] = np.linalg.norm(perturbation.flatten()*255, 2)\n",
    "        acc[i+1] = 1. - (i+1.)/adv_size\n",
    "\n",
    "    eps = np.sort(eps)\n",
    "    #print(eps[0],acc[0],eps[6],acc[6],eps[7],acc[7])\n",
    "    \n",
    "    return eps, acc\n",
    "\n",
    "    #print(\"image label\", adversaries_rl[i,1], \"peturb label\", adversaries_rl[i,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "adversaries_rl_ewa = np.load(\"./experiment_attacks/exp/adversarials_batch_0_full_with-aug_max-ent.npy\", allow_pickle=True)\n",
    "eps_ewa, acc_ewa = find_epsilons(adversaries_rl_ewa)\n",
    "#eps_ewa\n",
    "\n",
    "adversaries_rl_2 = np.load(\"./experiment_attacks/exp/adversarials_batch_0_energy_with_aug.npy\", allow_pickle=True) #_energy_with_aug\n",
    "eps_2, acc_2 = find_epsilons(adversaries_rl_2)\n",
    "\n",
    "adversaries_rl_2a = np.load(\"./experiment_attacks/exp/adversarials_batch_0.npy\", allow_pickle=True) #_energy_with_aug\n",
    "eps_2a, acc_2a = find_epsilons(adversaries_rl_2a)\n",
    "\n",
    "\n",
    "adversaries_rl_b = np.load(\"./experiment_attacks/exp/adversarials_batch_0_with_aug.npy\", allow_pickle=True)\n",
    "eps_b, acc_b = find_epsilons(adversaries_rl_b)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eps_b, acc_b * 100, label=\"Baseline\")\n",
    "\n",
    "plt.plot(eps_ewa, acc_ewa * 100, label=\"JEM\")\n",
    "\n",
    "plt.plot(eps_2, acc_2 * 100, label=\"JEM-CRS\")\n",
    "\n",
    "plt.plot(eps_2a, acc_2a * 100, label=\"JEM-CRS-abs-dpx\")\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.xlim([0,300])\n",
    "\n",
    "plt.xlabel(\"epsilon\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Adversarial attack\")\n",
    "\n",
    "\n",
    "plt.savefig('experiment_output/images/adversarials.png')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "img = adversaries_rl[1,0]\n",
    "img = img.swapaxes(0,1)\n",
    "img = img.swapaxes(1,2)\n",
    "plt.imshow(img, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "img = adversaries_rl[1,2]\n",
    "img = img.swapaxes(0,1)\n",
    "img = img.swapaxes(1,2)\n",
    "plt.imshow(img, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "img = adversaries_rl[1,2] - adversaries_rl[1,0]\n",
    "img = img.swapaxes(0,1)\n",
    "img = img.swapaxes(1,2)\n",
    "plt.imshow(img, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversaries_rl[1,2] - adversaries_rl[1,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(eps_ewa[:40], acc_ewa[:40])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
