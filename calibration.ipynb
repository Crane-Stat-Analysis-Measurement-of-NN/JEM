{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import utils # from The Google Research Authors\n",
    "import torch as t, torch.nn as nn, torch.nn.functional as tnnF, torch.distributions as tdist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "#import ipdb\n",
    "import numpy as np\n",
    "import wideresnet # from The Google Research Authors\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "from tqdm import tqdm\n",
    "t.backends.cudnn.benchmark = True\n",
    "t.backends.cudnn.enabled = True\n",
    "seed = 1\n",
    "\n",
    "# images RGB 32x32\n",
    "im_sz = 32\n",
    "n_ch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random subset of data\n",
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Wide_ResNet\n",
    "# Uses The Google Research Authors, file wideresnet.py\n",
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm, dropout_rate=dropout_rate)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, n_classes)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energies if y=none\n",
    "# EBM energy calculated as logsumexp of logits\n",
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm, dropout_rate=dropout_rate, n_classes=n_classes)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            # gathers the logits along dim 1 with indeces y\n",
    "            return t.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various utilities\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def grad_norm(m):\n",
    "    total_norm = 0\n",
    "    for p in m.parameters():\n",
    "        param_grad = p.grad\n",
    "        if param_grad is not None:\n",
    "            param_norm = param_grad.data.norm(2) ** 2\n",
    "            total_norm += param_norm\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm.item()\n",
    "\n",
    "def grad_vals(m):\n",
    "    ps = []\n",
    "    for p in m.parameters():\n",
    "        if p.grad is not None:\n",
    "            ps.append(p.grad.data.view(-1))\n",
    "    ps = t.cat(ps)\n",
    "    return ps.mean().item(), ps.std(), ps.abs().mean(), ps.abs().std(), ps.abs().min(), ps.abs().max()\n",
    "\n",
    "def init_random(args, bs):\n",
    "    return t.FloatTensor(bs, n_ch, im_sz, im_sz).uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SGLD model and data/replay buffer\n",
    "# Images generated are added to a buffer and sampled with a probability (1-\\rho) for efficiency\n",
    "def get_model_and_buffer(args, device, sample_q):\n",
    "    model_cls = F if args.uncond else CCF\n",
    "    f = model_cls(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    if not args.uncond:\n",
    "        assert args.buffer_size % args.n_classes == 0, \"Buffer size must be divisible by args.n_classes\"\n",
    "    if args.load_path is None:\n",
    "        # make replay buffer\n",
    "        replay_buffer = init_random(args, args.buffer_size)\n",
    "        epoch=-1 #because it needs to start at 0\n",
    "    else:\n",
    "        print(f\"loading model from {args.load_path}\")\n",
    "        ckpt_dict = t.load(args.load_path)\n",
    "        f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "        #replay_buffer = ckpt_dict[\"replay_buffer\"]\n",
    "        replay_buffer = init_random(args, args.buffer_size)\n",
    "        #epoch = ckpt_dict[\"epoch\"]\n",
    "        epoch=48\n",
    "\n",
    "    f = f.to(device)\n",
    "    return f, replay_buffer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in chosen dataset from svhn, cifar10, cifar100\n",
    "def get_data(args):\n",
    "    if args.dataset == \"svhn\":\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "    else:\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.RandomHorizontalFlip(),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "        #transform_train = tr.Compose(\n",
    "        #    [tr.ToTensor()]\n",
    "        #)\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "    def dataset_fn(train, transform):\n",
    "        if args.dataset == \"cifar10\":\n",
    "            return tv.datasets.CIFAR10(root=args.data_root, transform=transform, download=True, train=train)\n",
    "        elif args.dataset == \"cifar100\":\n",
    "            return tv.datasets.CIFAR100(root=args.data_root, transform=transform, download=True, train=train)\n",
    "        else:\n",
    "            return tv.datasets.SVHN(root=args.data_root, transform=transform, download=True,\n",
    "                                    split=\"train\" if train else \"test\")\n",
    "\n",
    "    # get all training inds\n",
    "    full_train = dataset_fn(True, transform_train)\n",
    "    all_inds = list(range(len(full_train)))\n",
    "    # set seed\n",
    "    np.random.seed(1234)\n",
    "    # shuffle\n",
    "    np.random.shuffle(all_inds)\n",
    "    # seperate out validation set\n",
    "    if args.n_valid is not None:\n",
    "        valid_inds, train_inds = all_inds[:args.n_valid], all_inds[args.n_valid:]\n",
    "    else:\n",
    "        valid_inds, train_inds = [], all_inds\n",
    "    train_inds = np.array(train_inds)\n",
    "    train_labeled_inds = []\n",
    "    other_inds = []\n",
    "    train_labels = np.array([full_train[ind][1] for ind in train_inds])\n",
    "    if args.labels_per_class > 0:\n",
    "        for i in range(args.n_classes):\n",
    "            print(i)\n",
    "            train_labeled_inds.extend(train_inds[train_labels == i][:args.labels_per_class])\n",
    "            other_inds.extend(train_inds[train_labels == i][args.labels_per_class:])\n",
    "    else:\n",
    "        train_labeled_inds = train_inds\n",
    "\n",
    "    dset_train = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_inds)\n",
    "    dset_train_labeled = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_labeled_inds)\n",
    "    dset_valid = DataSubset(\n",
    "        dataset_fn(True, transform_test),\n",
    "        inds=valid_inds)\n",
    "    dload_train = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = DataLoader(dset_train_labeled, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = cycle(dload_train_labeled)\n",
    "    dset_test = dataset_fn(False, transform_test)\n",
    "    dload_valid = DataLoader(dset_valid, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    dload_test = DataLoader(dset_test, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    return dload_train, dload_train_labeled, dload_valid,dload_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine for SGLD generation of fake images\n",
    "def get_sample_q(args, device):\n",
    "    # setup initial data/buffers\n",
    "    def sample_p_0(replay_buffer, bs, y=None):\n",
    "        if len(replay_buffer) == 0:\n",
    "            return init_random(args, bs), []\n",
    "        buffer_size = len(replay_buffer) if y is None else len(replay_buffer) // args.n_classes\n",
    "        inds = t.randint(0, buffer_size, (bs,))\n",
    "        # if cond, convert inds to class conditional inds\n",
    "        if y is not None:\n",
    "            inds = y.cpu() * buffer_size + inds\n",
    "            assert not args.uncond, \"Can't drawn conditional samples without giving me y\"\n",
    "        buffer_samples = replay_buffer[inds]\n",
    "        random_samples = init_random(args, bs)\n",
    "        choose_random = (t.rand(bs) < args.reinit_freq).float()[:, None, None, None]\n",
    "        samples = choose_random * random_samples + (1 - choose_random) * buffer_samples\n",
    "        return samples.to(device), inds\n",
    "\n",
    "    # actual SGLD\n",
    "    def sample_q(f, replay_buffer, y=None, n_steps=args.n_steps):\n",
    "        \"\"\"this func takes in replay_buffer now so we have the option to sample from\n",
    "        scratch (i.e. replay_buffer==[]).  See test_wrn_ebm.py for example.\n",
    "        \"\"\"\n",
    "        # here f is CCF to calculate energies\n",
    "        # evaluate model, must set train back on later (TODO:but I dont need to train energies?)\n",
    "        f.eval()\n",
    "        # get batch size\n",
    "        bs = args.batch_size if y is None else y.size(0)\n",
    "        # generate initial samples and buffer inds of those samples (if buffer is used)\n",
    "        init_sample, buffer_inds = sample_p_0(replay_buffer, bs=bs, y=y)\n",
    "        x_k = t.autograd.Variable(init_sample, requires_grad=True)\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            # calculate \\parial E/\\partial x_{k-1}\n",
    "            f_prime = t.autograd.grad(f(x_k, y=y).sum(), [x_k], retain_graph=True)[0]\n",
    "            # x_k = x_{k-1} + \\alpha*\\parial E/\\partial x_{k-1} + \\theta * N\n",
    "            x_k.data += args.sgld_lr * f_prime + args.sgld_std * t.randn_like(x_k)\n",
    "        \n",
    "        # set self.training = True\n",
    "        f.train()\n",
    "        \n",
    "        # Returns a new Tensor, detached from the current graph\n",
    "        final_samples = x_k.detach()\n",
    "        \n",
    "        # update replay buffer\n",
    "        if len(replay_buffer) > 0:\n",
    "            replay_buffer[buffer_inds] = final_samples.cpu()\n",
    "        return final_samples\n",
    "    return sample_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid repeat code and maintanence. This is for the evaluations\n",
    "def eval_classification_inner(f,dload,device):\n",
    "    softmax=nn.Softmax(dim=1)\n",
    "    corrects, losses, logits_all = [], [], []\n",
    "    for x_p_d, y_p_d in dload:\n",
    "        x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "        logits = f.classify(x_p_d)\n",
    "        logits_all.extend(logits)\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().numpy()\n",
    "        losses.extend(loss)\n",
    "        \n",
    "        correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "        corrects.extend(correct)\n",
    "    logits_all=t.stack(logits_all)\n",
    "    logits=softmax(logits_all)\n",
    "    sms = logits.max(1)[0]\n",
    "    cali_vals=[(a,b.item()) for a,b in zip(corrects,sms)]\n",
    "    return corrects, losses, cali_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss and accuracy for periodic printout\n",
    "def eval_classification(f, dload, device):\n",
    "    corrects, losses, _ = eval_classification_inner(f,dload,device)\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    return correct, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate loss and accuracy for calibration\n",
    "def eval_with_calibration(f, dload, device):\n",
    "    corrects, losses, cali_vals = eval_classification_inner(f,dload,device)\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    return correct, loss, cali_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the calibration data to a file\n",
    "def save_calibration(filename,cali_vals):\n",
    "    with open(filename,\"w\") as f:\n",
    "        f.write(\"correct,softmax\\n\")\n",
    "        for i in cali_vals:\n",
    "            f.write(\"{},{}\\n\".format(i[0],i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track loss for convergence\n",
    "def loss_tracker(filename,epoch,loss,correct):\n",
    "    if not os.path.isfile(os.path.join(args.save_dir,filename)):\n",
    "        with open(os.path.join(args.save_dir,filename),'w') as of:\n",
    "            of.write(\"Epoch,Loss,Acc\\n\")\n",
    "            of.write(\"{},{},{}\\n\".format(epoch,loss,correct))\n",
    "    else:\n",
    "        with open(os.path.join(args.save_dir,filename),'a') as of:\n",
    "            of.write(\"{},{},{}\\n\".format(epoch,loss,correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint data\n",
    "def checkpoint(f, opt, epoch_no, tag, args, device):\n",
    "    f.cpu()\n",
    "    ckpt_dict = {\n",
    "        \"model_state_dict\": f.state_dict(),\n",
    "        'optimizer_state_dict': opt.state_dict(),\n",
    "        'epoch': epoch_no\n",
    "        #\"replay_buffer\": buffer\n",
    "    }\n",
    "    t.save(ckpt_dict, os.path.join(args.save_dir, tag))\n",
    "    #This line added so that the most recent ckpt will be saved as 'most_recent'\n",
    "    t.save(ckpt_dict, os.path.join(args.save_dir,'most_recent.pt'))\n",
    "    f.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the newest ckpt if not using the \"most_recent.pt\" file\n",
    "def nat_keys(word):\n",
    "    def atoi(c):\n",
    "        return int(c) if c.isdigit() else c\n",
    "    return [atoi(c) for c in re.split('(\\d+)',word)]\n",
    "\n",
    "def get_most_recent_ckpt(dir):\n",
    "    ckpt=sorted([i for i in os.listdir(dir) if 'ckpt'==i[:4]],key=nat_keys)[-1]\n",
    "    return os.path.join(dir,ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function adds or overwrites a file to the output dir named '0_readme.txt'\n",
    "#That file contains what we were hoping to do with that experiment\n",
    "def exp_purpose(words,filename='0_readme.txt'):\n",
    "    with open(os.path.join(args.save_dir,filename),'w') as f:\n",
    "        f.write(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for training\n",
    "# Uses args from class below\n",
    "def main(args):\n",
    "    #Does eval and saving for test, train, and valid\n",
    "    def eval_all_3(eval_type,with_tracker=False):\n",
    "        evs=['test', 'train', 'valid']\n",
    "        dls=[dload_test,dload_train,dload_valid]\n",
    "        for ev,dl in zip(evs,dls):\n",
    "            correct, loss, cv = eval_type(f, dload_test, device)\n",
    "            if eval_type==eval_with_calibration:\n",
    "                save_calibration(os.path.join(args.save_dir,f'cali_{ev}.csv'),cv)\n",
    "            if with_tracker:\n",
    "                loss_tracker(f'track_{ev}.csv',epoch,loss,correct)\n",
    "            print(f\"{ev}: Epoch {epoch}: Valid Loss {loss}, Valid Acc {correct}\")\n",
    "            \n",
    "    utils.makedirs(args.save_dir)\n",
    "    with open(f'{args.save_dir}/params.txt', 'w') as f:\n",
    "        json.dump(args.__dict__, f)\n",
    "    if args.print_to_log:\n",
    "        sys.stdout = open(f'{args.save_dir}/log.txt', 'w')\n",
    "\n",
    "    t.manual_seed(seed)\n",
    "    if t.cuda.is_available():\n",
    "        t.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # store purpose of experiment\n",
    "    exp_purpose(\"Calibration for no sgld with augmentation.\")\n",
    "    \n",
    "    # datasets\n",
    "    dload_train, dload_train_labeled, dload_valid, dload_test = get_data(args)\n",
    "    \n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #MODEL\n",
    "    sample_q = get_sample_q(args, device)\n",
    "    f, replay_buffer, epoch = get_model_and_buffer(args, device, sample_q)\n",
    "\n",
    "    epoch =48\n",
    "    \n",
    "    sqrt = lambda x: int(t.sqrt(t.Tensor([x])))\n",
    "    plot = lambda p, x: tv.utils.save_image(t.clamp(x, -1, 1), p, normalize=True, nrow=sqrt(x.size(0)))\n",
    "\n",
    "    # optimizer\n",
    "    params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "    if args.optimizer == \"adam\":\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        optim = t.optim.SGD(params, lr=args.lr, momentum=.9, weight_decay=args.weight_decay)\n",
    "\n",
    "    best_valid_acc = 0.0\n",
    "    cur_iter = 0\n",
    "    \n",
    "    f.eval()\n",
    "    with t.no_grad():\n",
    "        eval_all_3(eval_with_calibration,with_tracker=True,)\n",
    "    f.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "# defaults for paper\n",
    "# --lr .0001 --dataset cifar10 --optimizer adam --p_x_weight 1.0 --p_y_given_x_weight 1.0 \n",
    "# --p_x_y_weight 0.0 --sigma .03 --width 10 --depth 28 --save_dir /YOUR/SAVE/DIR \n",
    "# --plot_uncond --warmup_iters 1000\n",
    "#\n",
    "# Regression\n",
    "# {\"dataset\": \"cifar10\", \"data_root\": \"../data\", \"lr\": 0.0001, \"decay_epochs\": [160, 180], \n",
    "# \"decay_rate\": 0.3, \"clf_only\": false, \"labels_per_class\": -1, \"optimizer\": \"adam\", \n",
    "# \"batch_size\": 64, \"n_epochs\": 200, \"warmup_iters\": 1000, \"p_x_weight\": 1.0, \n",
    "# \"p_y_given_x_weight\": 1.0, \"p_x_y_weight\": 0.0, \"dropout_rate\": 0.0, \"sigma\": 0.03, \n",
    "# \"weight_decay\": 0.0, \"norm\": null, \"n_steps\": 20, \"width\": 10, \"depth\": 28, \"uncond\": false, \n",
    "# \"class_cond_p_x_sample\": false, \"buffer_size\": 10000, \"reinit_freq\": 0.05, \"sgld_lr\": 1.0, \n",
    "# \"sgld_std\": 0.01, \"save_dir\": \"./savedir\", \"ckpt_every\": 10, \"eval_every\": 1, \n",
    "# \"print_every\": 100, \"load_path\": null, \"print_to_log\": false, \"plot_cond\": false, \n",
    "#\"plot_uncond\": true, \"n_valid\": 5000, \"n_classes\": 10}\n",
    "class train_args():\n",
    "    def __init__(self, param_dict):\n",
    "        # set defaults\n",
    "        self.dataset = \"cifar10\" #, choices=[\"cifar10\", \"svhn\", \"cifar100\"])\n",
    "        self.n_classes = 100 if self.dataset == \"cifar100\" else 10\n",
    "        self.data_root = \"../data\" \n",
    "        # optimization\n",
    "        self.lr = 1e-4\n",
    "        self.decay_epochs = [160, 180] # help=\"decay learning rate by decay_rate at these epochs\")\n",
    "        self.decay_rate = .3 # help=\"learning rate decay multiplier\")\n",
    "        self.clf_only = False #action=\"store_true\", help=\"If set, then only train the classifier\")\n",
    "        self.labels_per_class = -1# help=\"number of labeled examples per class, if zero then use all labels\")\n",
    "        self.optimizer = \"adam\" #choices=[\"adam\", \"sgd\"], default=\"adam\")\n",
    "        self.batch_size = 64\n",
    "        self.n_epochs = 200\n",
    "        self.warmup_iters = -1 # help=\"number of iters to linearly increase learning rate, if -1 then no warmmup\")\n",
    "        # loss weighting\n",
    "        self.p_x_weight = 1.\n",
    "        self.p_y_given_x_weight = 1.\n",
    "        self.p_x_y_weight = 0.\n",
    "        # regularization\n",
    "        self.dropout_rate = 0.0\n",
    "        self.sigma = 3e-2 # help=\"stddev of gaussian noise to add to input, .03 works but .1 is more stable\")\n",
    "        self.weight_decay = 0.0\n",
    "        # network\n",
    "        self.norm = None # choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"], help=\"norm to add to weights, none works fine\")\n",
    "        # EBM specific\n",
    "        self.n_steps = 20 # help=\"number of steps of SGLD per iteration, 100 works for short-run, 20 works for PCD\")\n",
    "        self.width = 10 # help=\"WRN width parameter\")\n",
    "        self.depth = 28 # help=\"WRN depth parameter\")\n",
    "        self.uncond = False # \"store_true\" # help=\"If set, then the EBM is unconditional\")\n",
    "        self.class_cond_p_x_sample = False #, action=\"store_true\", help=\"If set we sample from p(y)p(x|y), othewise sample from p(x),\" \"Sample quality higher if set, but classification accuracy better if not.\")\n",
    "        self.buffer_size = 10000\n",
    "        self.reinit_freq = .05\n",
    "        self.sgld_lr = 1.0\n",
    "        self.sgld_std = 1e-2\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.ckpt_every = 10 # help=\"Epochs between checkpoint save\")\n",
    "        self.eval_every = 1 # help=\"Epochs between evaluation\")\n",
    "        self.print_every = 100 # help=\"Iterations between print\")\n",
    "        self.load_path = None # path for checkpoint to load\n",
    "        self.print_to_log = False #\", action=\"store_true\", help=\"If true, directs std-out to log file\")\n",
    "        self.plot_cond = False #\", action=\"store_true\", help=\"If set, save class-conditional samples\")\n",
    "        self.plot_uncond = False #\", action=\"store_true\", help=\"If set, save unconditional samples\")\n",
    "        self.n_valid = 5000\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg warmup_iters -1 lr 0.0001\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "| Wide-Resnet 28x10\n",
      "loading model from energy_with_aug_last.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Epoch 48: Valid Loss 0.3322734832763672, Valid Acc 0.9054999947547913\n",
      "train: Epoch 48: Valid Loss 0.3308701515197754, Valid Acc 0.9067000150680542\n",
      "valid: Epoch 48: Valid Loss 0.32728850841522217, Valid Acc 0.9089000225067139\n"
     ]
    }
   ],
   "source": [
    "# setup change from defaults\n",
    "# For paper defaults\n",
    "# Added best_valid_ckpt load\n",
    "inline_parms = {\"lr\": .0001, \"dataset\": \"cifar10\", \"optimizer\": \"adam\", \"save_dir\": './chris2/', \\\n",
    "                \"p_x_weight\": 0, \"p_y_given_x_weight\": 1.0, \"p_x_y_weight\": 0.0, \\\n",
    "                \"sigma\": .03, \"width\": 10, \"depth\": 28, \"plot_uncond\": False, \\\n",
    "                \"uncond\": True, \"decay_epochs\": [], \\\n",
    "                \"load_path\": 'energy_with_aug_last.pt', \\\n",
    "                 \"n_epochs\": 1} # added loading from checkpoint \"load_path\": './exp2/best_valid_ckpt.pt',\n",
    "                                  #\"warmup_iters\": 1000,\n",
    "\n",
    "# instantiate\n",
    "args = train_args(inline_parms)\n",
    "\n",
    "print(\"arg warmup_iters\", args.warmup_iters, \"lr\", args.lr)\n",
    "\n",
    "# run\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
