{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "%matplotlib inline\n",
    "import utils\n",
    "import torch as t, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import wideresnet\n",
    "import pdb\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Sampling\n",
    "from tqdm import tqdm\n",
    "t.backends.cudnn.benchmark = True\n",
    "t.backends.cudnn.enabled = True\n",
    "seed = 1\n",
    "im_sz = 32\n",
    "n_ch = 3\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, 10)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            return t.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def init_random(bs):\n",
    "    return t.FloatTensor(bs, 3, 32, 32).uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_p_0(device, replay_buffer, bs, y=None):\n",
    "    if len(replay_buffer) == 0:\n",
    "        return init_random(bs), []\n",
    "    buffer_size = len(replay_buffer) if y is None else len(replay_buffer) // n_classes\n",
    "    inds = t.randint(0, buffer_size, (bs,))\n",
    "    # if cond, convert inds to class conditional inds\n",
    "    if y is not None:\n",
    "        inds = y.cpu() * buffer_size + inds\n",
    "        assert not args.uncond, \"Can't drawn conditional samples without giving me y\"\n",
    "    buffer_samples = replay_buffer[inds]\n",
    "    random_samples = init_random(bs)\n",
    "    choose_random = (t.rand(bs) < args.reinit_freq).float()[:, None, None, None]\n",
    "    samples = choose_random * random_samples + (1 - choose_random) * buffer_samples\n",
    "    return samples.to(device), inds\n",
    "\n",
    "\n",
    "def sample_q(args, device, f, replay_buffer, y=None):\n",
    "    \"\"\"this func takes in replay_buffer now so we have the option to sample from\n",
    "    scratch (i.e. replay_buffer==[]).  See test_wrn_ebm.py for example.\n",
    "    \"\"\"\n",
    "    f.eval()\n",
    "    # get batch size\n",
    "    bs = args.batch_size if y is None else y.size(0)\n",
    "    # generate initial samples and buffer inds of those samples (if buffer is used)\n",
    "    init_sample, buffer_inds = sample_p_0(device, replay_buffer, bs=bs, y=y)\n",
    "    x_k = t.autograd.Variable(init_sample, requires_grad=True)\n",
    "    # sgld\n",
    "    for k in range(args.n_steps):\n",
    "        f_prime = t.autograd.grad(f(x_k, y=y).sum(), [x_k], retain_graph=True)[0]\n",
    "        x_k.data += args.sgld_lr * f_prime + args.sgld_std * t.randn_like(x_k)\n",
    "    f.train()\n",
    "    final_samples = x_k.detach()\n",
    "    # update replay buffer\n",
    "    if len(replay_buffer) > 0:\n",
    "        replay_buffer[buffer_inds] = final_samples.cpu()\n",
    "    return final_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncond_samples(f, args, device, save=True):\n",
    "    sqrt = lambda x: int(t.sqrt(t.Tensor([x])))\n",
    "    plot = lambda p, x: tv.utils.save_image(t.clamp(x, -1, 1), p, normalize=True, nrow=sqrt(x.size(0)))\n",
    "\n",
    "    replay_buffer = t.FloatTensor(args.buffer_size, 3, 32, 32).uniform_(-1, 1)\n",
    "    for i in range(args.n_sample_steps):\n",
    "        samples = sample_q(args, device, f, replay_buffer)\n",
    "        if i % args.print_every == 0 and save:\n",
    "            plot('{}/samples_{}.png'.format(args.save_dir, i), samples)\n",
    "        print(i)\n",
    "    return replay_buffer\n",
    "\n",
    "def cond_samples(f, replay_buffer, args, device, fresh=False):\n",
    "    sqrt = lambda x: int(t.sqrt(t.Tensor([x])))\n",
    "    plot = lambda p, x: tv.utils.save_image(t.clamp(x, -1, 1), p, normalize=True, nrow=sqrt(x.size(0)))\n",
    "\n",
    "    if fresh:\n",
    "        replay_buffer = uncond_samples(f, args, device, save=False)\n",
    "    n_it = replay_buffer.size(0) // 100\n",
    "    all_y = []\n",
    "    for i in range(n_it):\n",
    "        x = replay_buffer[i * 100: (i + 1) * 100].to(device)\n",
    "        y = f.classify(x).max(1)[1]\n",
    "        all_y.append(y)\n",
    "\n",
    "    all_y = t.cat(all_y, 0)\n",
    "    each_class = [replay_buffer[all_y == l] for l in range(10)]\n",
    "    print([len(c) for c in each_class])\n",
    "    for i in range(100):\n",
    "        this_im = []\n",
    "        for l in range(10):\n",
    "            this_l = each_class[l][i * 10: (i + 1) * 10]\n",
    "            this_im.append(this_l)\n",
    "        this_im = t.cat(this_im, 0)\n",
    "        if this_im.size(0) > 0:\n",
    "            plot('{}/samples_{}.png'.format(args.save_dir, i), this_im)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_hist(f, args, device):\n",
    "    #from matplotlib import pyplot as plt\n",
    "    #import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    plt.switch_backend('agg')\n",
    "    def sample(x, n_steps=args.n_steps):\n",
    "        x_k = t.autograd.Variable(x.clone(), requires_grad=True)\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "            x_k.data += f_prime + 1e-2 * t.randn_like(x_k)\n",
    "        final_samples = x_k.detach()\n",
    "        return final_samples\n",
    "    def grad_norm(x):\n",
    "        x_k = t.autograd.Variable(x, requires_grad=True)\n",
    "        f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "        grad = f_prime.view(x.size(0), -1)\n",
    "        return grad.norm(p=2, dim=1)\n",
    "    def score_fn(x):\n",
    "        if args.score_fn == \"px\":\n",
    "            return f(x).detach().cpu()\n",
    "        elif args.score_fn == \"py\":\n",
    "            return nn.Softmax()(f.classify(x)).max(1)[0].detach().cpu()\n",
    "        elif args.score_fn == \"pxgrad\":\n",
    "            return -t.log(grad_norm(x).detach().cpu())\n",
    "        elif args.score_fn == \"refine\":\n",
    "            init_score = f(x)\n",
    "            x_r = sample(x)\n",
    "            final_score = f(x_r)\n",
    "            delta = init_score - final_score\n",
    "            return delta.detach().cpu()\n",
    "        elif args.score_fn == \"refinegrad\":\n",
    "            init_score = -grad_norm(x).detach()\n",
    "            x_r = sample(x)\n",
    "            final_score = -grad_norm(x_r).detach()\n",
    "            delta = init_score - final_score\n",
    "            return delta.detach().cpu()\n",
    "        elif args.score_fn == \"refinel2\":\n",
    "            x_r = sample(x)\n",
    "            norm = (x - x_r).view(x.size(0), -1).norm(p=2, dim=1)\n",
    "            return -norm.detach().cpu()\n",
    "        else:\n",
    "            return f.classify(x).max(1)[0].detach().cpu()\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "    datasets = {\n",
    "        \"cifar10\": tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=False),\n",
    "        \"svhn\": tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"test\"),\n",
    "        \"cifar100\":tv.datasets.CIFAR100(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "    }\n",
    "    score_dict = {}\n",
    "    for dataset_name in args.datasets:\n",
    "        print(dataset_name)\n",
    "        dataset = datasets[dataset_name]\n",
    "        dataloader = DataLoader(dataset, batch_size=100, shuffle=True, num_workers=4, drop_last=False)\n",
    "        this_scores = []\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            scores = score_fn(x)\n",
    "            #print(scores.mean())\n",
    "            this_scores.extend(scores.numpy())\n",
    "        score_dict[dataset_name] = this_scores\n",
    "\n",
    "    for name, scores in score_dict.items():\n",
    "        plt.hist(scores, label=name, bins=100, density=True, alpha=.5)\n",
    "    plt.legend()\n",
    "    plt.savefig(args.save_dir + \"/fig.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OODAUC(f, args, device):\n",
    "    print(\"OOD Evaluation\")\n",
    "\n",
    "    def grad_norm(x):\n",
    "        x_k = t.autograd.Variable(x, requires_grad=True)\n",
    "        f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "        grad = f_prime.view(x.size(0), -1)\n",
    "        return grad.norm(p=2, dim=1)\n",
    "\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "\n",
    "    dset_real = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "    dload_real = DataLoader(dset_real, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "    if args.ood_dataset == \"svhn\":\n",
    "        dset_fake = tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"test\")\n",
    "    elif args.ood_dataset == \"cifar_100\":\n",
    "        dset_fake = tv.datasets.CIFAR100(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "    elif args.ood_dataset == \"celeba\":\n",
    "        #dset_fake = tv.datasets.ImageFolder(root=\"/scratch/gobi1/gwohl/CelebA/splits\",\n",
    "        dset_fake = tv.datasets.CelebA(root=\"../data\",\n",
    "                                            transform=tr.Compose([tr.Resize(32),\n",
    "                                                       tr.ToTensor(),\n",
    "                                                       tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "                                                       lambda x: x + args.sigma * t.randn_like(x)]),\n",
    "                                            download=True)\n",
    "    else:\n",
    "        dset_fake = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "\n",
    "    dload_fake = DataLoader(dset_fake, batch_size=100, shuffle=True, num_workers=4, drop_last=False)\n",
    "    print(len(dload_real), len(dload_fake))\n",
    "    real_scores = []\n",
    "    print(\"Real scores...\")\n",
    "\n",
    "    def score_fn(x):\n",
    "        if args.score_fn == \"px\":\n",
    "            return f(x).detach().cpu()\n",
    "        elif args.score_fn == \"py\":\n",
    "            return nn.Softmax()(f.classify(x)).max(1)[0].detach().cpu()\n",
    "        else:\n",
    "            return -grad_norm(x).detach().cpu()\n",
    "\n",
    "    for x, _ in dload_real:\n",
    "        x = x.to(device)\n",
    "        scores = score_fn(x)\n",
    "        real_scores.append(scores.numpy())\n",
    "        print(scores.mean())\n",
    "    fake_scores = []\n",
    "    print(\"Fake scores...\")\n",
    "    if args.ood_dataset == \"cifar_interp\":\n",
    "        last_batch = None\n",
    "        for i, (x, _) in enumerate(dload_fake):\n",
    "            x = x.to(device)\n",
    "            if i > 0:\n",
    "                x_mix = (x + last_batch) / 2 + args.sigma * t.randn_like(x)\n",
    "                scores = score_fn(x_mix)\n",
    "                fake_scores.append(scores.numpy())\n",
    "                print(scores.mean())\n",
    "            last_batch = x\n",
    "    else:\n",
    "        for i, (x, _) in enumerate(dload_fake):\n",
    "            x = x.to(device)\n",
    "            scores = score_fn(x)\n",
    "            fake_scores.append(scores.numpy())\n",
    "            print(scores.mean())\n",
    "    real_scores = np.concatenate(real_scores)\n",
    "    fake_scores = np.concatenate(fake_scores)\n",
    "    real_labels = np.ones_like(real_scores)\n",
    "    fake_labels = np.zeros_like(fake_scores)\n",
    "    import sklearn.metrics\n",
    "    scores = np.concatenate([real_scores, fake_scores])\n",
    "    labels = np.concatenate([real_labels, fake_labels])\n",
    "    score = sklearn.metrics.roc_auc_score(labels, scores)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clf(f, args, device):\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + t.randn_like(x) * args.sigma]\n",
    "    )\n",
    "\n",
    "    def sample(x, n_steps=args.n_steps):\n",
    "        x_k = t.autograd.Variable(x.clone(), requires_grad=True)\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "            x_k.data += f_prime + 1e-2 * t.randn_like(x_k)\n",
    "        final_samples = x_k.detach()\n",
    "        return final_samples\n",
    "\n",
    "    if args.dataset == \"cifar_train\":\n",
    "        dset = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=True)\n",
    "    elif args.dataset == \"cifar_test\":\n",
    "        dset = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "    elif args.dataset == \"svhn_train\":\n",
    "        dset = tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"train\")\n",
    "    else:  # args.dataset == \"svhn_test\":\n",
    "        dset = tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"test\")\n",
    "\n",
    "    dload = DataLoader(dset, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "    corrects, losses, pys, preds = [], [], [], []\n",
    "    for x_p_d, y_p_d in tqdm(dload):\n",
    "        x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "        if args.n_steps > 0:\n",
    "            x_p_d = sample(x_p_d)\n",
    "        logits = f.classify(x_p_d)\n",
    "        py = nn.Softmax()(f.classify(x_p_d)).max(1)[0].detach().cpu().numpy()\n",
    "        loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().detach().numpy()\n",
    "        losses.extend(loss)\n",
    "        correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "        corrects.extend(correct)\n",
    "        pys.extend(py)\n",
    "        preds.extend(logits.max(1)[1].cpu().numpy())\n",
    "\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    t.save({\"losses\": losses, \"corrects\": corrects, \"pys\": pys}, os.path.join(args.save_dir, \"vals.pt\"))\n",
    "    print(loss, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pri_energy(f, args, device):\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + t.randn_like(x) * args.sigma]\n",
    "    )\n",
    "\n",
    "    def sample(x, n_steps=args.n_steps):\n",
    "        x_k = t.autograd.Variable(x.clone(), requires_grad=True)\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "            x_k.data += f_prime + 1e-2 * t.randn_like(x_k)\n",
    "        final_samples = x_k.detach()\n",
    "        return final_samples\n",
    "\n",
    "    if args.dataset == \"cifar_train\":\n",
    "        dset = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=True)\n",
    "    elif args.dataset == \"cifar_test\":\n",
    "        dset = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "    elif args.dataset == \"svhn_train\":\n",
    "        dset = tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"train\")\n",
    "    else:  # args.dataset == \"svhn_test\":\n",
    "        dset = tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"test\")\n",
    "\n",
    "    dload = DataLoader(dset, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "    energies, corrects, losses, pys, preds = [], [], [], [], []\n",
    "    for x_p_d, y_p_d in tqdm(dload):\n",
    "        x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "        if args.n_steps > 0:\n",
    "            x_p_d = sample(x_p_d)\n",
    "        logits = f.classify(x_p_d)\n",
    "\n",
    "        \n",
    "        py = nn.Softmax()(f.classify(x_p_d)).max(1)[0].detach().cpu().numpy()\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().detach().numpy()\n",
    "        losses.extend(loss)\n",
    "        \n",
    "        correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()        \n",
    "        corrects.extend(correct)\n",
    "\n",
    "        energy = logits.logsumexp(dim=1, keepdim=False).cpu().detach().numpy()\n",
    "        energies.extend(energy)\n",
    "        \n",
    "\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    \n",
    "    e_mean = np.mean(energies)\n",
    "    e_var = np.var(energies)\n",
    "    \n",
    "    print(e_mean, e_var, np.sqrt(e_var))\n",
    "    \n",
    "    # save energies in a text file\n",
    "    import pandas as pd     \n",
    "    pd.DataFrame(energies).to_csv(os.path.join(args.save_dir, \"energies.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    utils.makedirs(args.save_dir)\n",
    "    if args.print_to_log:\n",
    "        sys.stdout = open(f'{args.save_dir}/log.txt', 'w')\n",
    "\n",
    "    t.manual_seed(seed)\n",
    "    if t.cuda.is_available():\n",
    "        t.cuda.manual_seed_all(seed)\n",
    "\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model_cls = F if args.uncond else CCF\n",
    "    f = model_cls(args.depth, args.width, args.norm)\n",
    "    print(f\"loading model from {args.load_path}\")\n",
    "\n",
    "    # load em up\n",
    "    ckpt_dict = t.load(args.load_path)\n",
    "    f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "    replay_buffer = ckpt_dict[\"replay_buffer\"]\n",
    "\n",
    "    f = f.to(device)\n",
    "\n",
    "    if args.eval == \"OOD\":\n",
    "        OODAUC(f, args, device)\n",
    "\n",
    "    if args.eval == \"test_clf\":\n",
    "        test_clf(f, args, device)\n",
    "        \n",
    "    if args.eval == \"pri_energy\":\n",
    "        pri_energy(f, args, device)\n",
    "        \n",
    "    if args.eval == \"cond_samples\":\n",
    "        cond_samples(f, replay_buffer, args, device, args.fresh_samples)\n",
    "\n",
    "    if args.eval == \"uncond_samples\":\n",
    "        uncond_samples(f, args, device)\n",
    "\n",
    "    if args.eval == \"logp_hist\":\n",
    "        logp_hist(f, args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "class eval_args():\n",
    "    def __init__(self, param_dict):\n",
    "        self.eval = \"OOD\" #, type=str, choices=[\"uncond_samples\", \"cond_samples\", \"logp_hist\", \"OOD\", \"test_clf\", \"pri_energy\"])\n",
    "        self.score_fn = \"px\" #, type=str, choices=[\"px\", \"py\", \"pxgrad\"], help=\"For OODAUC, chooses what score function we use.\")\n",
    "        self.ood_dataset = \"svhn\" #, type=str, choices=[\"svhn\", \"cifar_interp\", \"cifar_100\", \"celeba\"], help=\"Chooses which dataset to compare against for OOD\")\n",
    "        self.dataset = \"cifar_test\" #, type=str, choices=[\"cifar_train\", \"cifar_test\", \"svhn_test\", \"svhn_train\"], help=\"Dataset to use when running test_clf for classification accuracy\")\n",
    "        self.datasets = [] # help=\"The datasets you wanna use to generate a log p(x) histogram\")\n",
    "        # optimization\n",
    "        self.batch_size = 64\n",
    "        # regularization\n",
    "        self.sigma = 3e-2\n",
    "        # network\n",
    "        self.norm = None #, choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"])\n",
    "        # EBM specific\n",
    "        self.n_steps = 20 # help=\"number of steps of SGLD per iteration, 100 works for short-run, 20 works for PCD\")\n",
    "        self.width = 10 # help=\"WRN width parameter\")\n",
    "        self.depth = 28 # help=\"WRN depth parameter\")\n",
    "        self.uncond = False # \"store_true\" # help=\"If set, then the EBM is unconditional\")\n",
    "        self.class_cond_p_x_sample = False #, action=\"store_true\", help=\"If set we sample from p(y)p(x|y), othewise sample from p(x),\" \"Sample quality higher if set, but classification accuracy better if not.\")\n",
    "        self.buffer_size = 0\n",
    "        self.reinit_freq = .05\n",
    "        self.sgld_lr = 1.0\n",
    "        self.sgld_std = 1e-2\n",
    "        # logging + evaluation\n",
    "        self.save_dir = 'output'\n",
    "        self.print_every = 100\n",
    "        self.n_sample_steps = 100\n",
    "        self.load_path = None\n",
    "        self.print_to_log = False\n",
    "        self.fresh_samples = False #\", action=\"store_true\", help=\"If set, then we generate a new replay buffer from scratch for conditional sampling,\"  \"Will be much slower.\")\n",
    "        self.datasets = []\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup change from defaults\n",
    "# For paper defaults\n",
    "# Added best_valid_ckpt load\n",
    "#inline_parms = {\"load_path\": \"./savedir/best_valid_ckpt.pt\", \"eval\": \"test_clf\", \"dataset\": \"cifar_test\"}\n",
    "#inline_parms = {\"load_path\": \"./savedir/best_valid_ckpt.pt\", \"eval\": \"pri_energy\", \"dataset\": \"cifar_train\", \"save_dir\": \"output_train\"}\n",
    "inline_parms = {\"load_path\": \"./fullrun/best_valid_ckpt.pt\", \"eval\": \"logp_hist\", \"datasets\": [\"cifar10\", \"svhn\"], \"save_dir\": \"output_hist\"}\n",
    "\n",
    "# instantiate\n",
    "args = eval_args(inline_parms)\n",
    "\n",
    "print(\"sgld_lr\", args.sgld_lr, args.datasets)\n",
    "\n",
    "# run\n",
    "main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis values \n",
    "x = [5, 2, 9, 4, 7] \n",
    "  \n",
    "# Y-axis values \n",
    "y = [10, 5, 8, 4, 2] \n",
    "  \n",
    "# Function to plot \n",
    "plt.plot(x, y) \n",
    "  \n",
    "# function to show the plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "from numpy import genfromtxt\n",
    "\n",
    "\n",
    "energies_train = genfromtxt('output_train/energies.csv', delimiter=',')\n",
    "energies_test = genfromtxt('output_test/energies.csv', delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=False, figsize=(15, 6))\n",
    "\n",
    "dat = energies_train[:,1]\n",
    "mean_dat = np.mean(dat)\n",
    "mean_var = np.var(dat)\n",
    "print(mean_dat, np.sqrt(mean_var))\n",
    "ax1.hist(dat, bins = np.arange(0.0, 3.0, 0.004)) \n",
    "#ax1.hist(dat)\n",
    "ax1.set_title(\"Train data histogram\") \n",
    "ax1.set_xlabel('Energy')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "dat = energies_test[:,1]\n",
    "mean_dat = np.mean(dat)\n",
    "mean_var = np.var(dat)\n",
    "print(mean_dat, np.sqrt(mean_var))\n",
    "ax2.hist(dat, bins = np.arange(0.0, 3.0, 0.004), color='green') \n",
    "#ax1.hist(dat)\n",
    "ax2.set_title(\"Test data histogram\") \n",
    "ax2.set_xlabel('Energy')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "#show the plots\n",
    "plt.savefig('JEM_hist_energy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
