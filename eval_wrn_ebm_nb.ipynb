{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import utils\n",
    "import torch as t, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import wideresnet\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Sampling\n",
    "from tqdm import tqdm\n",
    "t.backends.cudnn.benchmark = True\n",
    "t.backends.cudnn.enabled = True\n",
    "seed = 1\n",
    "im_sz = 32\n",
    "n_ch = 3\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, 10)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            return t.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def init_random(bs):\n",
    "    return t.FloatTensor(bs, 3, 32, 32).uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_p_0(device, replay_buffer, bs, y=None):\n",
    "    if len(replay_buffer) == 0:\n",
    "        return init_random(bs), []\n",
    "    buffer_size = len(replay_buffer) if y is None else len(replay_buffer) // n_classes\n",
    "    inds = t.randint(0, buffer_size, (bs,))\n",
    "    # if cond, convert inds to class conditional inds\n",
    "    if y is not None:\n",
    "        inds = y.cpu() * buffer_size + inds\n",
    "        assert not args.uncond, \"Can't drawn conditional samples without giving me y\"\n",
    "    buffer_samples = replay_buffer[inds]\n",
    "    random_samples = init_random(bs)\n",
    "    choose_random = (t.rand(bs) < args.reinit_freq).float()[:, None, None, None]\n",
    "    samples = choose_random * random_samples + (1 - choose_random) * buffer_samples\n",
    "    return samples.to(device), inds\n",
    "\n",
    "\n",
    "def sample_q(args, device, f, replay_buffer, y=None):\n",
    "    \"\"\"this func takes in replay_buffer now so we have the option to sample from\n",
    "    scratch (i.e. replay_buffer==[]).  See test_wrn_ebm.py for example.\n",
    "    \"\"\"\n",
    "    f.eval()\n",
    "    # get batch size\n",
    "    bs = args.batch_size if y is None else y.size(0)\n",
    "    # generate initial samples and buffer inds of those samples (if buffer is used)\n",
    "    init_sample, buffer_inds = sample_p_0(device, replay_buffer, bs=bs, y=y)\n",
    "    x_k = t.autograd.Variable(init_sample, requires_grad=True)\n",
    "    # sgld\n",
    "    for k in range(args.n_steps):\n",
    "        f_prime = t.autograd.grad(f(x_k, y=y).sum(), [x_k], retain_graph=True)[0]\n",
    "        x_k.data += args.sgld_lr * f_prime + args.sgld_std * t.randn_like(x_k)\n",
    "    f.train()\n",
    "    final_samples = x_k.detach()\n",
    "    # update replay buffer\n",
    "    if len(replay_buffer) > 0:\n",
    "        replay_buffer[buffer_inds] = final_samples.cpu()\n",
    "    return final_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncond_samples(f, args, device, save=True):\n",
    "    sqrt = lambda x: int(t.sqrt(t.Tensor([x])))\n",
    "    plot = lambda p, x: tv.utils.save_image(t.clamp(x, -1, 1), p, normalize=True, nrow=sqrt(x.size(0)))\n",
    "\n",
    "    replay_buffer = t.FloatTensor(args.buffer_size, 3, 32, 32).uniform_(-1, 1)\n",
    "    for i in range(args.n_sample_steps):\n",
    "        samples = sample_q(args, device, f, replay_buffer)\n",
    "        if i % args.print_every == 0 and save:\n",
    "            plot('{}/samples_{}.png'.format(args.save_dir, i), samples)\n",
    "        print(i)\n",
    "    return replay_buffer\n",
    "\n",
    "def cond_samples(f, replay_buffer, args, device, fresh=False):\n",
    "    sqrt = lambda x: int(t.sqrt(t.Tensor([x])))\n",
    "    plot = lambda p, x: tv.utils.save_image(t.clamp(x, -1, 1), p, normalize=True, nrow=sqrt(x.size(0)))\n",
    "\n",
    "    if fresh:\n",
    "        replay_buffer = uncond_samples(f, args, device, save=False)\n",
    "    n_it = replay_buffer.size(0) // 100\n",
    "    all_y = []\n",
    "    for i in range(n_it):\n",
    "        x = replay_buffer[i * 100: (i + 1) * 100].to(device)\n",
    "        y = f.classify(x).max(1)[1]\n",
    "        all_y.append(y)\n",
    "\n",
    "    all_y = t.cat(all_y, 0)\n",
    "    each_class = [replay_buffer[all_y == l] for l in range(10)]\n",
    "    print([len(c) for c in each_class])\n",
    "    for i in range(100):\n",
    "        this_im = []\n",
    "        for l in range(10):\n",
    "            this_l = each_class[l][i * 10: (i + 1) * 10]\n",
    "            this_im.append(this_l)\n",
    "        this_im = t.cat(this_im, 0)\n",
    "        if this_im.size(0) > 0:\n",
    "            plot('{}/samples_{}.png'.format(args.save_dir, i), this_im)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_hist(f, args, device):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    plt.switch_backend('agg')\n",
    "    def sample(x, n_steps=args.n_steps):\n",
    "        x_k = t.autograd.Variable(x.clone(), requires_grad=True)\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "            x_k.data += f_prime + 1e-2 * t.randn_like(x_k)\n",
    "        final_samples = x_k.detach()\n",
    "        return final_samples\n",
    "    def grad_norm(x):\n",
    "        x_k = t.autograd.Variable(x, requires_grad=True)\n",
    "        f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "        grad = f_prime.view(x.size(0), -1)\n",
    "        return grad.norm(p=2, dim=1)\n",
    "    def score_fn(x):\n",
    "        if args.score_fn == \"px\":\n",
    "            return f(x).detach().cpu()\n",
    "        elif args.score_fn == \"py\":\n",
    "            return nn.Softmax()(f.classify(x)).max(1)[0].detach().cpu()\n",
    "        elif args.score_fn == \"pxgrad\":\n",
    "            return -t.log(grad_norm(x).detach().cpu())\n",
    "        elif args.score_fn == \"refine\":\n",
    "            init_score = f(x)\n",
    "            x_r = sample(x)\n",
    "            final_score = f(x_r)\n",
    "            delta = init_score - final_score\n",
    "            return delta.detach().cpu()\n",
    "        elif args.score_fn == \"refinegrad\":\n",
    "            init_score = -grad_norm(x).detach()\n",
    "            x_r = sample(x)\n",
    "            final_score = -grad_norm(x_r).detach()\n",
    "            delta = init_score - final_score\n",
    "            return delta.detach().cpu()\n",
    "        elif args.score_fn == \"refinel2\":\n",
    "            x_r = sample(x)\n",
    "            norm = (x - x_r).view(x.size(0), -1).norm(p=2, dim=1)\n",
    "            return -norm.detach().cpu()\n",
    "        else:\n",
    "            return f.classify(x).max(1)[0].detach().cpu()\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "    datasets = {\n",
    "        \"cifar10\": tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=False),\n",
    "        \"svhn\": tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"test\"),\n",
    "        \"cifar100\":tv.datasets.CIFAR100(root=\"../data\", transform=transform_test, download=True, train=False),\n",
    "        #\"celeba\": tv.datasets.ImageFolder(root=\"/scratch/gobi1/gwohl/CelebA/splits\",\n",
    "        \"celeba\": tv.datasets.CelebA(root=\"../data\",\n",
    "                                          transform=tr.Compose([tr.Resize(32),\n",
    "                                                                tr.ToTensor(),\n",
    "                                                                tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "                                                                lambda x: x + args.sigma * t.randn_like(x)]),\n",
    "                                          download=True)\n",
    "    }\n",
    "\n",
    "    score_dict = {}\n",
    "    for dataset_name in args.datasets:\n",
    "        print(dataset_name)\n",
    "        dataset = datasets[dataset_name]\n",
    "        dataloader = DataLoader(dataset, batch_size=100, shuffle=True, num_workers=4, drop_last=False)\n",
    "        this_scores = []\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            scores = score_fn(x)\n",
    "            print(scores.mean())\n",
    "            this_scores.extend(scores.numpy())\n",
    "        score_dict[dataset_name] = this_scores\n",
    "\n",
    "    for name, scores in score_dict.items():\n",
    "        plt.hist(scores, label=name, bins=100, normed=True, alpha=.5)\n",
    "    plt.legend()\n",
    "    plt.savefig(args.save_dir + \"/fig.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OODAUC(f, args, device):\n",
    "    print(\"OOD Evaluation\")\n",
    "\n",
    "    def grad_norm(x):\n",
    "        x_k = t.autograd.Variable(x, requires_grad=True)\n",
    "        f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "        grad = f_prime.view(x.size(0), -1)\n",
    "        return grad.norm(p=2, dim=1)\n",
    "\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "\n",
    "    dset_real = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "    dload_real = DataLoader(dset_real, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "    if args.ood_dataset == \"svhn\":\n",
    "        dset_fake = tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"test\")\n",
    "    elif args.ood_dataset == \"cifar_100\":\n",
    "        dset_fake = tv.datasets.CIFAR100(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "    elif args.ood_dataset == \"celeba\":\n",
    "        #dset_fake = tv.datasets.ImageFolder(root=\"/scratch/gobi1/gwohl/CelebA/splits\",\n",
    "        dset_fake = tv.datasets.CelebA(root=\"../data\",\n",
    "                                            transform=tr.Compose([tr.Resize(32),\n",
    "                                                       tr.ToTensor(),\n",
    "                                                       tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "                                                       lambda x: x + args.sigma * t.randn_like(x)]),\n",
    "                                            download=True)\n",
    "    else:\n",
    "        dset_fake = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "\n",
    "    dload_fake = DataLoader(dset_fake, batch_size=100, shuffle=True, num_workers=4, drop_last=False)\n",
    "    print(len(dload_real), len(dload_fake))\n",
    "    real_scores = []\n",
    "    print(\"Real scores...\")\n",
    "\n",
    "    def score_fn(x):\n",
    "        if args.score_fn == \"px\":\n",
    "            return f(x).detach().cpu()\n",
    "        elif args.score_fn == \"py\":\n",
    "            return nn.Softmax()(f.classify(x)).max(1)[0].detach().cpu()\n",
    "        else:\n",
    "            return -grad_norm(x).detach().cpu()\n",
    "\n",
    "    for x, _ in dload_real:\n",
    "        x = x.to(device)\n",
    "        scores = score_fn(x)\n",
    "        real_scores.append(scores.numpy())\n",
    "        print(scores.mean())\n",
    "    fake_scores = []\n",
    "    print(\"Fake scores...\")\n",
    "    if args.ood_dataset == \"cifar_interp\":\n",
    "        last_batch = None\n",
    "        for i, (x, _) in enumerate(dload_fake):\n",
    "            x = x.to(device)\n",
    "            if i > 0:\n",
    "                x_mix = (x + last_batch) / 2 + args.sigma * t.randn_like(x)\n",
    "                scores = score_fn(x_mix)\n",
    "                fake_scores.append(scores.numpy())\n",
    "                print(scores.mean())\n",
    "            last_batch = x\n",
    "    else:\n",
    "        for i, (x, _) in enumerate(dload_fake):\n",
    "            x = x.to(device)\n",
    "            scores = score_fn(x)\n",
    "            fake_scores.append(scores.numpy())\n",
    "            print(scores.mean())\n",
    "    real_scores = np.concatenate(real_scores)\n",
    "    fake_scores = np.concatenate(fake_scores)\n",
    "    real_labels = np.ones_like(real_scores)\n",
    "    fake_labels = np.zeros_like(fake_scores)\n",
    "    import sklearn.metrics\n",
    "    scores = np.concatenate([real_scores, fake_scores])\n",
    "    labels = np.concatenate([real_labels, fake_labels])\n",
    "    score = sklearn.metrics.roc_auc_score(labels, scores)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clf(f, args, device):\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + t.randn_like(x) * args.sigma]\n",
    "    )\n",
    "\n",
    "    def sample(x, n_steps=args.n_steps):\n",
    "        x_k = t.autograd.Variable(x.clone(), requires_grad=True)\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "            x_k.data += f_prime + 1e-2 * t.randn_like(x_k)\n",
    "        final_samples = x_k.detach()\n",
    "        return final_samples\n",
    "\n",
    "    if args.dataset == \"cifar_train\":\n",
    "        dset = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=True)\n",
    "    elif args.dataset == \"cifar_test\":\n",
    "        dset = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "    elif args.dataset == \"svhn_train\":\n",
    "        dset = tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"train\")\n",
    "    else:  # args.dataset == \"svhn_test\":\n",
    "        dset = tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"test\")\n",
    "\n",
    "    dload = DataLoader(dset, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "    corrects, losses, pys, preds = [], [], [], []\n",
    "    for x_p_d, y_p_d in tqdm(dload):\n",
    "        x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "        if args.n_steps > 0:\n",
    "            x_p_d = sample(x_p_d)\n",
    "        logits = f.classify(x_p_d)\n",
    "        py = nn.Softmax()(f.classify(x_p_d)).max(1)[0].detach().cpu().numpy()\n",
    "        loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().detach().numpy()\n",
    "        losses.extend(loss)\n",
    "        correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "        corrects.extend(correct)\n",
    "        pys.extend(py)\n",
    "        preds.extend(logits.max(1)[1].cpu().numpy())\n",
    "\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    t.save({\"losses\": losses, \"corrects\": corrects, \"pys\": pys}, os.path.join(args.save_dir, \"vals.pt\"))\n",
    "    print(loss, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pri_energy(f, args, device):\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + t.randn_like(x) * args.sigma]\n",
    "    )\n",
    "\n",
    "    def sample(x, n_steps=args.n_steps):\n",
    "        x_k = t.autograd.Variable(x.clone(), requires_grad=True)\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "            x_k.data += f_prime + 1e-2 * t.randn_like(x_k)\n",
    "        final_samples = x_k.detach()\n",
    "        return final_samples\n",
    "\n",
    "    if args.dataset == \"cifar_train\":\n",
    "        dset = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=True)\n",
    "    elif args.dataset == \"cifar_test\":\n",
    "        dset = tv.datasets.CIFAR10(root=\"../data\", transform=transform_test, download=True, train=False)\n",
    "    elif args.dataset == \"svhn_train\":\n",
    "        dset = tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"train\")\n",
    "    else:  # args.dataset == \"svhn_test\":\n",
    "        dset = tv.datasets.SVHN(root=\"../data\", transform=transform_test, download=True, split=\"test\")\n",
    "\n",
    "    dload = DataLoader(dset, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "    energies, corrects, losses, pys, preds = [], [], [], [], []\n",
    "    for x_p_d, y_p_d in tqdm(dload):\n",
    "        x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "        if args.n_steps > 0:\n",
    "            x_p_d = sample(x_p_d)\n",
    "        logits = f.classify(x_p_d)\n",
    "\n",
    "        \n",
    "        py = nn.Softmax()(f.classify(x_p_d)).max(1)[0].detach().cpu().numpy()\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().detach().numpy()\n",
    "        losses.extend(loss)\n",
    "        \n",
    "        correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()        \n",
    "        corrects.extend(correct)\n",
    "\n",
    "        energy = logits.logsumexp(dim=1, keepdim=False).cpu().detach().numpy()\n",
    "        energies.extend(energy)\n",
    "        \n",
    "\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    \n",
    "    e_mean = np.mean(energies)\n",
    "    e_var = np.var(energies)\n",
    "    \n",
    "    print(e_mean, e_var, np.sqrt(e_var))\n",
    "    \n",
    "    # save energies in a text file\n",
    "    import pandas as pd     \n",
    "    pd.DataFrame(energies).to_csv(os.path.join(args.save_dir, \"energies.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    utils.makedirs(args.save_dir)\n",
    "    if args.print_to_log:\n",
    "        sys.stdout = open(f'{args.save_dir}/log.txt', 'w')\n",
    "\n",
    "    t.manual_seed(seed)\n",
    "    if t.cuda.is_available():\n",
    "        t.cuda.manual_seed_all(seed)\n",
    "\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model_cls = F if args.uncond else CCF\n",
    "    f = model_cls(args.depth, args.width, args.norm)\n",
    "    print(f\"loading model from {args.load_path}\")\n",
    "\n",
    "    # load em up\n",
    "    ckpt_dict = t.load(args.load_path)\n",
    "    f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "    replay_buffer = ckpt_dict[\"replay_buffer\"]\n",
    "\n",
    "    f = f.to(device)\n",
    "\n",
    "    if args.eval == \"OOD\":\n",
    "        OODAUC(f, args, device)\n",
    "\n",
    "    if args.eval == \"test_clf\":\n",
    "        test_clf(f, args, device)\n",
    "        \n",
    "    if args.eval == \"pri_energy\":\n",
    "        pri_energy(f, args, device)\n",
    "        \n",
    "    if args.eval == \"cond_samples\":\n",
    "        cond_samples(f, replay_buffer, args, device, args.fresh_samples)\n",
    "\n",
    "    if args.eval == \"uncond_samples\":\n",
    "        uncond_samples(f, args, device)\n",
    "\n",
    "    if args.eval == \"logp_hist\":\n",
    "        logp_hist(f, args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "class eval_args():\n",
    "    def __init__(self, param_dict):\n",
    "        self.eval = \"OOD\" #, type=str, choices=[\"uncond_samples\", \"cond_samples\", \"logp_hist\", \"OOD\", \"test_clf\", \"pri_energy\"])\n",
    "        self.score_fn = \"px\" #, type=str, choices=[\"px\", \"py\", \"pxgrad\"], help=\"For OODAUC, chooses what score function we use.\")\n",
    "        self.ood_dataset = \"svhn\" #, type=str, choices=[\"svhn\", \"cifar_interp\", \"cifar_100\", \"celeba\"], help=\"Chooses which dataset to compare against for OOD\")\n",
    "        self.dataset = \"cifar_test\" #, type=str, choices=[\"cifar_train\", \"cifar_test\", \"svhn_test\", \"svhn_train\"], help=\"Dataset to use when running test_clf for classification accuracy\")\n",
    "        self.datasets = [] # help=\"The datasets you wanna use to generate a log p(x) histogram\")\n",
    "        # optimization\n",
    "        self.batch_size = 64\n",
    "        # regularization\n",
    "        self.sigma = 3e-2\n",
    "        # network\n",
    "        self.norm = None #, choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"])\n",
    "        # EBM specific\n",
    "        self.n_steps = 20 # help=\"number of steps of SGLD per iteration, 100 works for short-run, 20 works for PCD\")\n",
    "        self.width = 10 # help=\"WRN width parameter\")\n",
    "        self.depth = 28 # help=\"WRN depth parameter\")\n",
    "        self.uncond = False # \"store_true\" # help=\"If set, then the EBM is unconditional\")\n",
    "        self.class_cond_p_x_sample = False #, action=\"store_true\", help=\"If set we sample from p(y)p(x|y), othewise sample from p(x),\" \"Sample quality higher if set, but classification accuracy better if not.\")\n",
    "        self.buffer_size = 0\n",
    "        self.reinit_freq = .05\n",
    "        self.sgld_lr = 1.0\n",
    "        self.sgld_std = 1e-2\n",
    "        # logging + evaluation\n",
    "        self.save_dir = 'output'\n",
    "        self.print_every = 100\n",
    "        self.n_sample_steps = 100\n",
    "        self.load_path = None\n",
    "        self.print_to_log = False\n",
    "        self.fresh_samples = False #\", action=\"store_true\", help=\"If set, then we generate a new replay buffer from scratch for conditional sampling,\"  \"Will be much slower.\")\n",
    "\n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgld_lr 1.0\n",
      "| Wide-Resnet 28x10\n",
      "loading model from ./savedir/best_valid_ckpt.pt\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib64/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "100%|██████████| 500/500 [1:13:36<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.264603 0.1936955 0.4401085\n"
     ]
    }
   ],
   "source": [
    "# setup change from defaults\n",
    "# For paper defaults\n",
    "# Added best_valid_ckpt load\n",
    "#inline_parms = {\"load_path\": \"./savedir/best_valid_ckpt.pt\", \"eval\": \"test_clf\", \"dataset\": \"cifar_test\"}\n",
    "inline_parms = {\"load_path\": \"./savedir/best_valid_ckpt.pt\", \"eval\": \"pri_energy\", \"dataset\": \"cifar_train\", \"save_dir\": \"output_train\"}\n",
    "\n",
    "# instantiate\n",
    "args = eval_args(inline_parms)\n",
    "\n",
    "print(\"sgld_lr\", args.sgld_lr)\n",
    "\n",
    "# run\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2645778014287894 0.4401404292214238\n",
      "1.2668830184039683 0.4438280493055083\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "from numpy import genfromtxt\n",
    "\n",
    "\n",
    "energies_train = genfromtxt('output_train/energies.csv', delimiter=',')\n",
    "energies_test = genfromtxt('output_test/energies.csv', delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=False, figsize=(15, 6))\n",
    "\n",
    "dat = energies_train[:,1]\n",
    "mean_dat = np.mean(dat)\n",
    "mean_var = np.var(dat)\n",
    "print(mean_dat, np.sqrt(mean_var))\n",
    "ax1.hist(dat, bins = np.arange(0.0, 3.0, 0.004)) \n",
    "#ax1.hist(dat)\n",
    "ax1.set_title(\"Train data histogram\") \n",
    "ax1.set_xlabel('Energy')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "dat = energies_test[:,1]\n",
    "mean_dat = np.mean(dat)\n",
    "mean_var = np.var(dat)\n",
    "print(mean_dat, np.sqrt(mean_var))\n",
    "ax2.hist(dat, bins = np.arange(0.0, 3.0, 0.004), color='green') \n",
    "#ax1.hist(dat)\n",
    "ax2.set_title(\"Test data histogram\") \n",
    "ax2.set_xlabel('Energy')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "#show the plots\n",
    "plt.savefig('JEM_hist_energy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
