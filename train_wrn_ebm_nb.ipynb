{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import utils # from The Google Research Authors\n",
    "import torch as t, torch.nn as nn, torch.nn.functional as tnnF, torch.distributions as tdist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "#import ipdb\n",
    "import numpy as np\n",
    "import wideresnet # from The Google Research Authors\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "from tqdm import tqdm\n",
    "t.backends.cudnn.benchmark = True\n",
    "t.backends.cudnn.enabled = True\n",
    "seed = 1\n",
    "\n",
    "# images RGB 32x32\n",
    "im_sz = 32\n",
    "n_ch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random subset of data\n",
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Wide_ResNet\n",
    "# Uses The Google Research Authors, file wideresnet.py\n",
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm, dropout_rate=dropout_rate)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, n_classes)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energies if y=none\n",
    "# EBM energy calculated as logsumexp of logits\n",
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm, dropout_rate=dropout_rate, n_classes=n_classes)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            # gathers the logits along dim 1 with indeces y\n",
    "            return t.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various utilities\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def grad_norm(m):\n",
    "    total_norm = 0\n",
    "    for p in m.parameters():\n",
    "        param_grad = p.grad\n",
    "        if param_grad is not None:\n",
    "            param_norm = param_grad.data.norm(2) ** 2\n",
    "            total_norm += param_norm\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm.item()\n",
    "\n",
    "def grad_vals(m):\n",
    "    ps = []\n",
    "    for p in m.parameters():\n",
    "        if p.grad is not None:\n",
    "            ps.append(p.grad.data.view(-1))\n",
    "    ps = t.cat(ps)\n",
    "    return ps.mean().item(), ps.std(), ps.abs().mean(), ps.abs().std(), ps.abs().min(), ps.abs().max()\n",
    "\n",
    "def init_random(args, bs):\n",
    "    return t.FloatTensor(bs, n_ch, im_sz, im_sz).uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SGLD model and data/replay buffer\n",
    "# Images generated are added to a buffer and sampled with a probability (1-\\rho) for efficiency\n",
    "def get_model_and_buffer(args, device, sample_q):\n",
    "    model_cls = F if args.uncond else CCF\n",
    "    f = model_cls(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    if not args.uncond:\n",
    "        assert args.buffer_size % args.n_classes == 0, \"Buffer size must be divisible by args.n_classes\"\n",
    "    if args.load_path is None:\n",
    "        # make replay buffer\n",
    "        replay_buffer = init_random(args, args.buffer_size)\n",
    "        epoch=-1 #because it needs to start at 0\n",
    "    else:\n",
    "        print(f\"loading model from {args.load_path}\")\n",
    "        ckpt_dict = t.load(args.load_path)\n",
    "        f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "        #replay_buffer = ckpt_dict[\"replay_buffer\"]\n",
    "        replay_buffer = init_random(args, args.buffer_size)\n",
    "        epoch = ckpt_dict[\"epoch\"]\n",
    "\n",
    "    f = f.to(device)\n",
    "    return f, replay_buffer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in chosen dataset from svhn, cifar10, cifar100\n",
    "def get_data(args):\n",
    "    if args.dataset == \"svhn\":\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "    else:\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.RandomHorizontalFlip(),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "        #transform_train = tr.Compose(\n",
    "        #    [tr.ToTensor()]\n",
    "        #)\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "    def dataset_fn(train, transform):\n",
    "        if args.dataset == \"cifar10\":\n",
    "            return tv.datasets.CIFAR10(root=args.data_root, transform=transform, download=True, train=train)\n",
    "        elif args.dataset == \"cifar100\":\n",
    "            return tv.datasets.CIFAR100(root=args.data_root, transform=transform, download=True, train=train)\n",
    "        else:\n",
    "            return tv.datasets.SVHN(root=args.data_root, transform=transform, download=True,\n",
    "                                    split=\"train\" if train else \"test\")\n",
    "\n",
    "    # get all training inds\n",
    "    full_train = dataset_fn(True, transform_train)\n",
    "    all_inds = list(range(len(full_train)))\n",
    "    # set seed\n",
    "    np.random.seed(1234)\n",
    "    # shuffle\n",
    "    np.random.shuffle(all_inds)\n",
    "    # seperate out validation set\n",
    "    if args.n_valid is not None:\n",
    "        valid_inds, train_inds = all_inds[:args.n_valid], all_inds[args.n_valid:]\n",
    "    else:\n",
    "        valid_inds, train_inds = [], all_inds\n",
    "    train_inds = np.array(train_inds)\n",
    "    train_labeled_inds = []\n",
    "    other_inds = []\n",
    "    train_labels = np.array([full_train[ind][1] for ind in train_inds])\n",
    "    if args.labels_per_class > 0:\n",
    "        for i in range(args.n_classes):\n",
    "            print(i)\n",
    "            train_labeled_inds.extend(train_inds[train_labels == i][:args.labels_per_class])\n",
    "            other_inds.extend(train_inds[train_labels == i][args.labels_per_class:])\n",
    "    else:\n",
    "        train_labeled_inds = train_inds\n",
    "\n",
    "    dset_train = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_inds)\n",
    "    dset_train_labeled = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_labeled_inds)\n",
    "    dset_valid = DataSubset(\n",
    "        dataset_fn(True, transform_test),\n",
    "        inds=valid_inds)\n",
    "    dload_train = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = DataLoader(dset_train_labeled, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = cycle(dload_train_labeled)\n",
    "    dset_test = dataset_fn(False, transform_test)\n",
    "    dload_valid = DataLoader(dset_valid, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    dload_test = DataLoader(dset_test, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    return dload_train, dload_train_labeled, dload_valid,dload_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine for SGLD generation of fake images\n",
    "def get_sample_q(args, device):\n",
    "    # setup initial data/buffers\n",
    "    def sample_p_0(replay_buffer, bs, y=None):\n",
    "        if len(replay_buffer) == 0:\n",
    "            return init_random(args, bs), []\n",
    "        buffer_size = len(replay_buffer) if y is None else len(replay_buffer) // args.n_classes\n",
    "        inds = t.randint(0, buffer_size, (bs,))\n",
    "        # if cond, convert inds to class conditional inds\n",
    "        if y is not None:\n",
    "            inds = y.cpu() * buffer_size + inds\n",
    "            assert not args.uncond, \"Can't drawn conditional samples without giving me y\"\n",
    "        buffer_samples = replay_buffer[inds]\n",
    "        random_samples = init_random(args, bs)\n",
    "        choose_random = (t.rand(bs) < args.reinit_freq).float()[:, None, None, None]\n",
    "        samples = choose_random * random_samples + (1 - choose_random) * buffer_samples\n",
    "        return samples.to(device), inds\n",
    "\n",
    "    # actual SGLD\n",
    "    def sample_q(f, replay_buffer, y=None, n_steps=args.n_steps):\n",
    "        \"\"\"this func takes in replay_buffer now so we have the option to sample from\n",
    "        scratch (i.e. replay_buffer==[]).  See test_wrn_ebm.py for example.\n",
    "        \"\"\"\n",
    "        # here f is CCF to calculate energies\n",
    "        # evaluate model, must set train back on later (TODO:but I dont need to train energies?)\n",
    "        f.eval()\n",
    "        # get batch size\n",
    "        bs = args.batch_size if y is None else y.size(0)\n",
    "        # generate initial samples and buffer inds of those samples (if buffer is used)\n",
    "        init_sample, buffer_inds = sample_p_0(replay_buffer, bs=bs, y=y)\n",
    "        x_k = t.autograd.Variable(init_sample, requires_grad=True)\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            # calculate \\parial E/\\partial x_{k-1}\n",
    "            f_prime = t.autograd.grad(f(x_k, y=y).sum(), [x_k], retain_graph=True)[0]\n",
    "            # x_k = x_{k-1} + \\alpha*\\parial E/\\partial x_{k-1} + \\theta * N\n",
    "            x_k.data += args.sgld_lr * f_prime + args.sgld_std * t.randn_like(x_k)\n",
    "        \n",
    "        # set self.training = True\n",
    "        f.train()\n",
    "        \n",
    "        # Returns a new Tensor, detached from the current graph\n",
    "        final_samples = x_k.detach()\n",
    "        \n",
    "        # update replay buffer\n",
    "        if len(replay_buffer) > 0:\n",
    "            replay_buffer[buffer_inds] = final_samples.cpu()\n",
    "        return final_samples\n",
    "    return sample_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss and accuracy for periodic printout\n",
    "def eval_classification(f, dload, device):\n",
    "    corrects, losses = [], []\n",
    "    for x_p_d, y_p_d in dload:\n",
    "        x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "        logits = f.classify(x_p_d)\n",
    "        loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().numpy()\n",
    "        losses.extend(loss)\n",
    "        correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "        corrects.extend(correct)\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    return correct, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint data\n",
    "def checkpoint(f, opt, epoch_no, tag, args, device):\n",
    "    f.cpu()\n",
    "    ckpt_dict = {\n",
    "        \"model_state_dict\": f.state_dict(),\n",
    "        'optimizer_state_dict': opt.state_dict(),\n",
    "        'epoch': epoch_no,\n",
    "        #\"replay_buffer\": buffer\n",
    "    }\n",
    "    t.save(ckpt_dict, os.path.join(args.save_dir, tag))\n",
    "    #This line added so that the most recent ckpt will be saved as 'most_recent'\n",
    "    t.save(ckpt_dict, os.path.join(args.save_dir,'most_recent.pt'))\n",
    "    f.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track loss for convergence\n",
    "def loss_tracker(filename,save_dir,epoch,loss,correct):\n",
    "    if not os.path.isfile(os.path.join(args.save_dir,filename)):\n",
    "        with open(os.path.join(args.save_dir,filename),'w') as f:\n",
    "            f.write(\"Epoch,Loss,Acc\\n\")\n",
    "            f.write(\"{},{},{}\\n\".format(epoch,loss,correct))\n",
    "    else:\n",
    "        with open(os.path.join(args.save_dir,filename),'a') as f:\n",
    "            f.write(\"{},{},{}\\n\".format(epoch,loss,correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the newest ckpt if not using the \"most_recent.pt\" file\n",
    "def nat_keys(word):\n",
    "    def atoi(c):\n",
    "        return int(c) if c.isdigit() else c\n",
    "    return [atoi(c) for c in re.split('(\\d+)',word)]\n",
    "\n",
    "def get_most_recent_ckpt(dir):\n",
    "    ckpt=sorted([i for i in os.listdir(dir) if 'ckpt'==i[:4]],key=nat_keys)[-1]\n",
    "    return os.path.join(dir,ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function adds or overwrites a file to the output dir named '0_readme.txt'\n",
    "#That file contains what we were hoping to do with that experiment\n",
    "def exp_purpose(words,filename='0_readme.txt'):\n",
    "    with open(os.path.join(args.save_dir,filename),'w') as f:\n",
    "        f.write(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for training\n",
    "# Uses args from class below\n",
    "def main(args):\n",
    "    utils.makedirs(args.save_dir)\n",
    "    with open(f'{args.save_dir}/params.txt', 'w') as f:\n",
    "        json.dump(args.__dict__, f)\n",
    "    if args.print_to_log:\n",
    "        sys.stdout = open(f'{args.save_dir}/log.txt', 'w')\n",
    "\n",
    "    t.manual_seed(seed)\n",
    "    if t.cuda.is_available():\n",
    "        t.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # store purpose of experiment\n",
    "    exp_purpose(\"Get SGLD to train using the F model and simplest parameters.\")\n",
    "    \n",
    "    # datasets\n",
    "    dload_train, dload_train_labeled, dload_valid, dload_test = get_data(args)\n",
    "\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #MODEL\n",
    "    sample_q = get_sample_q(args, device)\n",
    "    f, replay_buffer, start_epoch = get_model_and_buffer(args, device, sample_q)\n",
    "\n",
    "    sqrt = lambda x: int(t.sqrt(t.Tensor([x])))\n",
    "    plot = lambda p, x: tv.utils.save_image(t.clamp(x, -1, 1), p, normalize=True, nrow=sqrt(x.size(0)))\n",
    "\n",
    "    # optimizer\n",
    "    params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "    if args.optimizer == \"adam\":\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        optim = t.optim.SGD(params, lr=args.lr, momentum=.9, weight_decay=args.weight_decay)\n",
    "\n",
    "    best_valid_acc = 0.0\n",
    "    cur_iter = 0\n",
    "    \n",
    "    f.eval()\n",
    "    with t.no_grad():\n",
    "        correct, loss = eval_classification(f, dload_train, device)\n",
    "        print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(start_epoch, loss, correct))\n",
    "    f.train()\n",
    "    \n",
    "    # loop over epochs\n",
    "    for epoch in range(start_epoch+1,(args.n_epochs+start_epoch)):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # decaying learning rate?\n",
    "        if epoch in args.decay_epochs:\n",
    "            for param_group in optim.param_groups:\n",
    "                new_lr = param_group['lr'] * args.decay_rate\n",
    "                param_group['lr'] = new_lr\n",
    "            print(\"Decaying lr to {}\".format(new_lr))\n",
    "            \n",
    "        # loop over data in batches\n",
    "        # x_p_d sample from dataset\n",
    "        for i, (x_p_d, _) in tqdm(enumerate(dload_train)):\n",
    "            # scale up lr to full over warmup time\n",
    "            if cur_iter <= args.warmup_iters:\n",
    "                lr = args.lr * cur_iter / float(args.warmup_iters)\n",
    "                for param_group in optim.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "\n",
    "            #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "            x_p_d = x_p_d.to(device)\n",
    "            x_lab, y_lab = dload_train_labeled.__next__()\n",
    "            x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "            # initialize loss\n",
    "            L = 0.\n",
    "            \n",
    "            # this maximizes log p(x) using SGLD\n",
    "            if args.p_x_weight > 0:  # maximize log p(x)\n",
    "                if args.class_cond_p_x_sample:\n",
    "                    assert not args.uncond, \"can only draw class-conditional samples if EBM is class-cond\"\n",
    "                    y_q = t.randint(0, args.n_classes, (args.batch_size,)).to(device)\n",
    "                    x_q = sample_q(f, replay_buffer, y=y_q)\n",
    "                else:\n",
    "                    # get data generated by SGLD\n",
    "                    # In paper x_q_shape torch.Size([64, 3, 32, 32])\n",
    "                    # Batch rgb 32x32\n",
    "                    x_q = sample_q(f, replay_buffer)  # sample from log-sumexp\n",
    "                    #print(\"x_q_shape\",x_q.shape)\n",
    "\n",
    "                # calculate energy for training data\n",
    "                fp_all = f(x_p_d)\n",
    "                \n",
    "                # calculate energy for SGLD generated sample\n",
    "                fq_all = f(x_q)\n",
    "                \n",
    "                # get means\n",
    "                fp = fp_all.mean()\n",
    "                fq = fq_all.mean()\n",
    "\n",
    "                # surrogate for the difference of expected value of \\partial Energy/\\partial x\n",
    "                # and \\partial Energy/\\partial x\n",
    "                # Need to maximize this, so preceded by minus\n",
    "                l_p_x = -(fp - fq)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    print('P(x) | {}:{:>d} f(x_p_d)={:>14.9f} f(x_q)={:>14.9f} d={:>14.9f}'.format(epoch, i, fp, fq,\n",
    "                                                                                                   fp - fq))\n",
    "                # add to loss\n",
    "                L += args.p_x_weight * l_p_x\n",
    "\n",
    "            # normal cross entropy loss function\n",
    "            if args.p_y_given_x_weight > 0:  # maximize log p(y | x)\n",
    "                logits = f.classify(x_lab)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                                 cur_iter,\n",
    "                                                                                 l_p_y_given_x.item(),\n",
    "                                                                                 acc.item()))\n",
    "                # add to loss\n",
    "                L += args.p_y_given_x_weight * l_p_y_given_x\n",
    "\n",
    "            #########################\n",
    "            # dont do this for paper\n",
    "            #########################\n",
    "            if args.p_x_y_weight > 0:  # maximize log p(x, y)\n",
    "                assert not args.uncond, \"this objective can only be trained for class-conditional EBM DUUUUUUUUHHHH!!!\"\n",
    "                x_q_lab = sample_q(f, replay_buffer, y=y_lab)\n",
    "                fp, fq = f(x_lab, y_lab).mean(), f(x_q_lab, y_lab).mean()\n",
    "                l_p_x_y = -(fp - fq)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    print('P(x, y) | {}:{:>d} f(x_p_d)={:>14.9f} f(x_q)={:>14.9f} d={:>14.9f}'.format(epoch, i, fp, fq,\n",
    "                                                                                                      fp - fq))\n",
    "\n",
    "                # add to loss\n",
    "                L += args.p_x_y_weight * l_p_x_y\n",
    "            #############################\n",
    "            # End dont do this for paper\n",
    "            #############################\n",
    "\n",
    "            # break if the loss diverged...easier for poppa to run experiments this way\n",
    "            if L.abs().item() > 1e8:\n",
    "                print(\"BAD BOIIIIIIIIII\")\n",
    "                1/0\n",
    "\n",
    "            # Optimize network using our loss function L\n",
    "            optim.zero_grad()            \n",
    "            L.backward()\n",
    "            optim.step()\n",
    "            cur_iter += 1\n",
    "\n",
    "            # Plot outputs\n",
    "            if cur_iter % 100 == 0:\n",
    "                if args.plot_uncond:\n",
    "                    if args.class_cond_p_x_sample:\n",
    "                        assert not args.uncond, \"can only draw class-conditional samples if EBM is class-cond\"\n",
    "                        y_q = t.randint(0, args.n_classes, (args.batch_size,)).to(device)\n",
    "                        x_q = sample_q(f, replay_buffer, y=y_q)\n",
    "                    else:\n",
    "                        x_q = sample_q(f, replay_buffer)\n",
    "                    plot('{}/x_q_{}_{:>06d}.png'.format(args.save_dir, epoch, i), x_q)\n",
    "                if args.plot_cond:  # generate class-conditional samples\n",
    "                    y = t.arange(0, args.n_classes)[None].repeat(args.n_classes, 1).transpose(1, 0).contiguous().view(-1).to(device)\n",
    "                    x_q_y = sample_q(f, replay_buffer, y=y)\n",
    "                    plot('{}/x_q_y{}_{:>06d}.png'.format(args.save_dir, epoch, i), x_q_y)\n",
    "\n",
    "        # do checkpointing\n",
    "        if epoch % args.ckpt_every == 0:\n",
    "            checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "\n",
    "        # Print performance assesment \n",
    "        if epoch % args.eval_every == 0 and (args.p_y_given_x_weight > 0 or args.p_x_y_weight > 0):\n",
    "            f.eval()\n",
    "            with t.no_grad():\n",
    "                # train set\n",
    "                correct, loss = eval_classification(f, dload_train, device)\n",
    "                print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "                loss_tracker('train.csv',args.save_dir,epoch,loss,correct)\n",
    "                # test set\n",
    "                correct, loss = eval_classification(f, dload_test, device)\n",
    "                print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "                loss_tracker('test.csv',args.save_dir,epoch,loss,correct)\n",
    "                # validation set\n",
    "                correct, loss = eval_classification(f, dload_valid, device)\n",
    "                print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "                loss_tracker('validation.csv',args.save_dir,epoch,loss,correct)\n",
    "                if correct > best_valid_acc:\n",
    "                    best_valid_acc = correct\n",
    "                    print(\"Best Valid!: {}\".format(correct))\n",
    "                    checkpoint(f, optim, epoch, f'best_valid_ckpt.pt', args, device)    \n",
    "            f.train()\n",
    "            \n",
    "        # do \"last\" checkpoint\n",
    "        checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "        \n",
    "    #Added to check the checkpoint\n",
    "    #return([f,optim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "# defaults for paper\n",
    "# --lr .0001 --dataset cifar10 --optimizer adam --p_x_weight 1.0 --p_y_given_x_weight 1.0 \n",
    "# --p_x_y_weight 0.0 --sigma .03 --width 10 --depth 28 --save_dir /YOUR/SAVE/DIR \n",
    "# --plot_uncond --warmup_iters 1000\n",
    "#\n",
    "# Regression\n",
    "# {\"dataset\": \"cifar10\", \"data_root\": \"../data\", \"lr\": 0.0001, \"decay_epochs\": [160, 180], \n",
    "# \"decay_rate\": 0.3, \"clf_only\": false, \"labels_per_class\": -1, \"optimizer\": \"adam\", \n",
    "# \"batch_size\": 64, \"n_epochs\": 200, \"warmup_iters\": 1000, \"p_x_weight\": 1.0, \n",
    "# \"p_y_given_x_weight\": 1.0, \"p_x_y_weight\": 0.0, \"dropout_rate\": 0.0, \"sigma\": 0.03, \n",
    "# \"weight_decay\": 0.0, \"norm\": null, \"n_steps\": 20, \"width\": 10, \"depth\": 28, \"uncond\": false, \n",
    "# \"class_cond_p_x_sample\": false, \"buffer_size\": 10000, \"reinit_freq\": 0.05, \"sgld_lr\": 1.0, \n",
    "# \"sgld_std\": 0.01, \"save_dir\": \"./savedir\", \"ckpt_every\": 10, \"eval_every\": 1, \n",
    "# \"print_every\": 100, \"load_path\": null, \"print_to_log\": false, \"plot_cond\": false, \n",
    "#\"plot_uncond\": true, \"n_valid\": 5000, \"n_classes\": 10}\n",
    "class train_args():\n",
    "    def __init__(self, param_dict):\n",
    "        # set defaults\n",
    "        self.dataset = \"cifar10\" #, choices=[\"cifar10\", \"svhn\", \"cifar100\"])\n",
    "        self.n_classes = 100 if self.dataset == \"cifar100\" else 10\n",
    "        self.data_root = \"../data\" \n",
    "        # optimization\n",
    "        self.lr = 1e-4\n",
    "        self.decay_epochs = [160, 180] # help=\"decay learning rate by decay_rate at these epochs\")\n",
    "        self.decay_rate = .3 # help=\"learning rate decay multiplier\")\n",
    "        self.clf_only = False #action=\"store_true\", help=\"If set, then only train the classifier\")\n",
    "        self.labels_per_class = -1# help=\"number of labeled examples per class, if zero then use all labels\")\n",
    "        self.optimizer = \"adam\" #choices=[\"adam\", \"sgd\"], default=\"adam\")\n",
    "        self.batch_size = 64\n",
    "        self.n_epochs = 200\n",
    "        self.warmup_iters = -1 # help=\"number of iters to linearly increase learning rate, if -1 then no warmmup\")\n",
    "        # loss weighting\n",
    "        self.p_x_weight = 1.\n",
    "        self.p_y_given_x_weight = 1.\n",
    "        self.p_x_y_weight = 0.\n",
    "        # regularization\n",
    "        self.dropout_rate = 0.0\n",
    "        self.sigma = 3e-2 # help=\"stddev of gaussian noise to add to input, .03 works but .1 is more stable\")\n",
    "        self.weight_decay = 0.0\n",
    "        # network\n",
    "        self.norm = None # choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"], help=\"norm to add to weights, none works fine\")\n",
    "        # EBM specific\n",
    "        self.n_steps = 20 # help=\"number of steps of SGLD per iteration, 100 works for short-run, 20 works for PCD\")\n",
    "        self.width = 10 # help=\"WRN width parameter\")\n",
    "        self.depth = 28 # help=\"WRN depth parameter\")\n",
    "        self.uncond = False # \"store_true\" # help=\"If set, then the EBM is unconditional\")\n",
    "        self.class_cond_p_x_sample = False #, action=\"store_true\", help=\"If set we sample from p(y)p(x|y), othewise sample from p(x),\" \"Sample quality higher if set, but classification accuracy better if not.\")\n",
    "        self.buffer_size = 10000\n",
    "        self.reinit_freq = .05\n",
    "        self.sgld_lr = 1.0\n",
    "        self.sgld_std = 1e-2\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.ckpt_every = 10 # help=\"Epochs between checkpoint save\")\n",
    "        self.eval_every = 1 # help=\"Epochs between evaluation\")\n",
    "        self.print_every = 100 # help=\"Iterations between print\")\n",
    "        self.load_path = None # path for checkpoint to load\n",
    "        self.print_to_log = False #\", action=\"store_true\", help=\"If true, directs std-out to log file\")\n",
    "        self.plot_cond = False #\", action=\"store_true\", help=\"If set, save class-conditional samples\")\n",
    "        self.plot_uncond = False #\", action=\"store_true\", help=\"If set, save unconditional samples\")\n",
    "        self.n_valid = 5000\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg warmup_iters -1 lr 0.0001\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-736577362ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-d9938c79e47a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdload_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdload_train_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdload_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdload_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-67fa5fdfcbe4>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mtrain_labeled_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mother_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_per_class\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-67fa5fdfcbe4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mtrain_labeled_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mother_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_per_class\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;31m# PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setup change from defaults\n",
    "# For paper defaults\n",
    "# Added best_valid_ckpt load\n",
    "inline_parms = {\"lr\": .0001, \"dataset\": \"cifar10\", \"optimizer\": \"adam\", \"save_dir\": './test_X-ent_SGLD/', \\\n",
    "                \"p_x_weight\": 1.0, \"p_y_given_x_weight\": 1.0, \"p_x_y_weight\": 0.0, \\\n",
    "                \"sigma\": .03, \"width\": 10, \"depth\": 28, \"plot_uncond\": False, \\\n",
    "                \"uncond\": True, \"decay_epochs\": [], \\\n",
    "                \"load_path\": './test_X-ent_SGLD/most_recent.pt', \\\n",
    "                 \"n_epochs\": 200} # added loading from checkpoint \"load_path\": './exp2/best_valid_ckpt.pt',\n",
    "                                  #\"warmup_iters\": 1000,\n",
    "\n",
    "# instantiate\n",
    "args = train_args(inline_parms)\n",
    "\n",
    "print(\"arg warmup_iters\", args.warmup_iters, \"lr\", args.lr)\n",
    "\n",
    "# run\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
