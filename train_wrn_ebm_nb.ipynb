{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import utils\n",
    "import torch as t, torch.nn as nn, torch.nn.functional as tnnF, torch.distributions as tdist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "#import ipdb\n",
    "import numpy as np\n",
    "import wideresnet\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "from tqdm import tqdm\n",
    "t.backends.cudnn.benchmark = True\n",
    "t.backends.cudnn.enabled = True\n",
    "seed = 1\n",
    "im_sz = 32\n",
    "n_ch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random subset of data\n",
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Wide_ResNet\n",
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm, dropout_rate=dropout_rate)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, n_classes)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energies if y=none\n",
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm, dropout_rate=dropout_rate, n_classes=n_classes)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            # gathers the logits along dim 1 with indeces y\n",
    "            return t.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def grad_norm(m):\n",
    "    total_norm = 0\n",
    "    for p in m.parameters():\n",
    "        param_grad = p.grad\n",
    "        if param_grad is not None:\n",
    "            param_norm = param_grad.data.norm(2) ** 2\n",
    "            total_norm += param_norm\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm.item()\n",
    "\n",
    "def grad_vals(m):\n",
    "    ps = []\n",
    "    for p in m.parameters():\n",
    "        if p.grad is not None:\n",
    "            ps.append(p.grad.data.view(-1))\n",
    "    ps = t.cat(ps)\n",
    "    return ps.mean().item(), ps.std(), ps.abs().mean(), ps.abs().std(), ps.abs().min(), ps.abs().max()\n",
    "\n",
    "def init_random(args, bs):\n",
    "    return t.FloatTensor(bs, n_ch, im_sz, im_sz).uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_buffer(args, device, sample_q):\n",
    "    model_cls = F if args.uncond else CCF\n",
    "    f = model_cls(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    if not args.uncond:\n",
    "        assert args.buffer_size % args.n_classes == 0, \"Buffer size must be divisible by args.n_classes\"\n",
    "    if args.load_path is None:\n",
    "        # make replay buffer\n",
    "        replay_buffer = init_random(args, args.buffer_size)\n",
    "    else:\n",
    "        print(f\"loading model from {args.load_path}\")\n",
    "        ckpt_dict = t.load(args.load_path)\n",
    "        f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "        replay_buffer = ckpt_dict[\"replay_buffer\"]\n",
    "\n",
    "    f = f.to(device)\n",
    "    return f, replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(args):\n",
    "    if args.dataset == \"svhn\":\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "    else:\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.RandomHorizontalFlip(),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "    def dataset_fn(train, transform):\n",
    "        if args.dataset == \"cifar10\":\n",
    "            return tv.datasets.CIFAR10(root=args.data_root, transform=transform, download=True, train=train)\n",
    "        elif args.dataset == \"cifar100\":\n",
    "            return tv.datasets.CIFAR100(root=args.data_root, transform=transform, download=True, train=train)\n",
    "        else:\n",
    "            return tv.datasets.SVHN(root=args.data_root, transform=transform, download=True,\n",
    "                                    split=\"train\" if train else \"test\")\n",
    "\n",
    "    # get all training inds\n",
    "    full_train = dataset_fn(True, transform_train)\n",
    "    all_inds = list(range(len(full_train)))\n",
    "    # set seed\n",
    "    np.random.seed(1234)\n",
    "    # shuffle\n",
    "    np.random.shuffle(all_inds)\n",
    "    # seperate out validation set\n",
    "    if args.n_valid is not None:\n",
    "        valid_inds, train_inds = all_inds[:args.n_valid], all_inds[args.n_valid:]\n",
    "    else:\n",
    "        valid_inds, train_inds = [], all_inds\n",
    "    train_inds = np.array(train_inds)\n",
    "    train_labeled_inds = []\n",
    "    other_inds = []\n",
    "    train_labels = np.array([full_train[ind][1] for ind in train_inds])\n",
    "    if args.labels_per_class > 0:\n",
    "        for i in range(args.n_classes):\n",
    "            print(i)\n",
    "            train_labeled_inds.extend(train_inds[train_labels == i][:args.labels_per_class])\n",
    "            other_inds.extend(train_inds[train_labels == i][args.labels_per_class:])\n",
    "    else:\n",
    "        train_labeled_inds = train_inds\n",
    "\n",
    "    dset_train = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_inds)\n",
    "    dset_train_labeled = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_labeled_inds)\n",
    "    dset_valid = DataSubset(\n",
    "        dataset_fn(True, transform_test),\n",
    "        inds=valid_inds)\n",
    "    dload_train = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = DataLoader(dset_train_labeled, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = cycle(dload_train_labeled)\n",
    "    dset_test = dataset_fn(False, transform_test)\n",
    "    dload_valid = DataLoader(dset_valid, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    dload_test = DataLoader(dset_test, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    return dload_train, dload_train_labeled, dload_valid,dload_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_q(args, device):\n",
    "    def sample_p_0(replay_buffer, bs, y=None):\n",
    "        if len(replay_buffer) == 0:\n",
    "            return init_random(args, bs), []\n",
    "        buffer_size = len(replay_buffer) if y is None else len(replay_buffer) // args.n_classes\n",
    "        inds = t.randint(0, buffer_size, (bs,))\n",
    "        # if cond, convert inds to class conditional inds\n",
    "        if y is not None:\n",
    "            inds = y.cpu() * buffer_size + inds\n",
    "            assert not args.uncond, \"Can't drawn conditional samples without giving me y\"\n",
    "        buffer_samples = replay_buffer[inds]\n",
    "        random_samples = init_random(args, bs)\n",
    "        choose_random = (t.rand(bs) < args.reinit_freq).float()[:, None, None, None]\n",
    "        samples = choose_random * random_samples + (1 - choose_random) * buffer_samples\n",
    "        return samples.to(device), inds\n",
    "\n",
    "    def sample_q(f, replay_buffer, y=None, n_steps=args.n_steps):\n",
    "        \"\"\"this func takes in replay_buffer now so we have the option to sample from\n",
    "        scratch (i.e. replay_buffer==[]).  See test_wrn_ebm.py for example.\n",
    "        \"\"\"\n",
    "        # here f is CCF to calculate energies\n",
    "        # evaluate model, must set train back on later (TODO:but I dont need to train energies?)\n",
    "        f.eval()\n",
    "        # get batch size\n",
    "        bs = args.batch_size if y is None else y.size(0)\n",
    "        # generate initial samples and buffer inds of those samples (if buffer is used)\n",
    "        init_sample, buffer_inds = sample_p_0(replay_buffer, bs=bs, y=y)\n",
    "        x_k = t.autograd.Variable(init_sample, requires_grad=True)\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            # calculate \\parial E/\\partial x_{k-1}\n",
    "            f_prime = t.autograd.grad(f(x_k, y=y).sum(), [x_k], retain_graph=True)[0]\n",
    "            # x_k = x_{k-1} + \\alpha*\\parial E/\\partial x_{k-1} + \\theta * N\n",
    "            x_k.data += args.sgld_lr * f_prime + args.sgld_std * t.randn_like(x_k)\n",
    "        \n",
    "        # set self.training = True\n",
    "        f.train()\n",
    "        # Returns a new Tensor, detached from the current graph\n",
    "        final_samples = x_k.detach()\n",
    "        # update replay buffer\n",
    "        if len(replay_buffer) > 0:\n",
    "            replay_buffer[buffer_inds] = final_samples.cpu()\n",
    "        return final_samples\n",
    "    return sample_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classification(f, dload, device):\n",
    "    corrects, losses = [], []\n",
    "    for x_p_d, y_p_d in dload:\n",
    "        x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "        logits = f.classify(x_p_d)\n",
    "        loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().numpy()\n",
    "        losses.extend(loss)\n",
    "        correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "        corrects.extend(correct)\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    return correct, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(f, buffer, tag, args, device):\n",
    "    f.cpu()\n",
    "    ckpt_dict = {\n",
    "        \"model_state_dict\": f.state_dict(),\n",
    "        \"replay_buffer\": buffer\n",
    "    }\n",
    "    t.save(ckpt_dict, os.path.join(args.save_dir, tag))\n",
    "    f.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for training\n",
    "def main(args):\n",
    "    utils.makedirs(args.save_dir)\n",
    "    with open(f'{args.save_dir}/params.txt', 'w') as f:\n",
    "        json.dump(args.__dict__, f)\n",
    "    if args.print_to_log:\n",
    "        sys.stdout = open(f'{args.save_dir}/log.txt', 'w')\n",
    "\n",
    "    t.manual_seed(seed)\n",
    "    if t.cuda.is_available():\n",
    "        t.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # datasets\n",
    "    dload_train, dload_train_labeled, dload_valid, dload_test = get_data(args)\n",
    "\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "    sample_q = get_sample_q(args, device)\n",
    "    f, replay_buffer = get_model_and_buffer(args, device, sample_q)\n",
    "\n",
    "    sqrt = lambda x: int(t.sqrt(t.Tensor([x])))\n",
    "    plot = lambda p, x: tv.utils.save_image(t.clamp(x, -1, 1), p, normalize=True, nrow=sqrt(x.size(0)))\n",
    "\n",
    "    # optimizer\n",
    "    params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "    if args.optimizer == \"adam\":\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        optim = t.optim.SGD(params, lr=args.lr, momentum=.9, weight_decay=args.weight_decay)\n",
    "\n",
    "    best_valid_acc = 0.0\n",
    "    cur_iter = 0\n",
    "    for epoch in range(args.n_epochs):\n",
    "        if epoch in args.decay_epochs:\n",
    "            for param_group in optim.param_groups:\n",
    "                new_lr = param_group['lr'] * args.decay_rate\n",
    "                param_group['lr'] = new_lr\n",
    "            print(\"Decaying lr to {}\".format(new_lr))\n",
    "            \n",
    "        # loop over data in batches\n",
    "        # x_p_d sample from dataset\n",
    "        for i, (x_p_d, _) in tqdm(enumerate(dload_train)):\n",
    "            if cur_iter <= args.warmup_iters:\n",
    "                lr = args.lr * cur_iter / float(args.warmup_iters)\n",
    "                for param_group in optim.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "\n",
    "            #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "            x_p_d = x_p_d.to(device)\n",
    "            x_lab, y_lab = dload_train_labeled.__next__()\n",
    "            x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "            # initialize loss\n",
    "            L = 0.\n",
    "            \n",
    "            # this maximizes log p(x) using SGLD\n",
    "            if args.p_x_weight > 0:  # maximize log p(x)\n",
    "                if args.class_cond_p_x_sample:\n",
    "                    assert not args.uncond, \"can only draw class-conditional samples if EBM is class-cond\"\n",
    "                    y_q = t.randint(0, args.n_classes, (args.batch_size,)).to(device)\n",
    "                    x_q = sample_q(f, replay_buffer, y=y_q)\n",
    "                else:\n",
    "                    # get data generated by SGLD\n",
    "                    # In paper x_q_shape torch.Size([64, 3, 32, 32])\n",
    "                    # Batch rgb 32x32\n",
    "                    x_q = sample_q(f, replay_buffer)  # sample from log-sumexp\n",
    "                    #print(\"x_q_shape\",x_q.shape)\n",
    "\n",
    "                # calculate energy for training data\n",
    "                fp_all = f(x_p_d)\n",
    "                \n",
    "                # calculate energy for SGLD generated sample\n",
    "                fq_all = f(x_q)\n",
    "                \n",
    "                # get means\n",
    "                fp = fp_all.mean()\n",
    "                fq = fq_all.mean()\n",
    "\n",
    "                # surrogate for the difference of expected value of \\partial Energy/\\partial x\n",
    "                # and \\partial Energy/\\partial x\n",
    "                # Need to maximize this, so preceeded by minus\n",
    "                l_p_x = -(fp - fq)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    print('P(x) | {}:{:>d} f(x_p_d)={:>14.9f} f(x_q)={:>14.9f} d={:>14.9f}'.format(epoch, i, fp, fq,\n",
    "                                                                                                   fp - fq))\n",
    "                # add to loss\n",
    "                L += args.p_x_weight * l_p_x\n",
    "\n",
    "            # normal cross entropy loss function\n",
    "            if args.p_y_given_x_weight > 0:  # maximize log p(y | x)\n",
    "                logits = f.classify(x_lab)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                                 cur_iter,\n",
    "                                                                                 l_p_y_given_x.item(),\n",
    "                                                                                 acc.item()))\n",
    "                # add to loss\n",
    "                L += args.p_y_given_x_weight * l_p_y_given_x\n",
    "\n",
    "            #########################\n",
    "            # dont do this for paper\n",
    "            #########################\n",
    "            if args.p_x_y_weight > 0:  # maximize log p(x, y)\n",
    "                assert not args.uncond, \"this objective can only be trained for class-conditional EBM DUUUUUUUUHHHH!!!\"\n",
    "                x_q_lab = sample_q(f, replay_buffer, y=y_lab)\n",
    "                fp, fq = f(x_lab, y_lab).mean(), f(x_q_lab, y_lab).mean()\n",
    "                l_p_x_y = -(fp - fq)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    print('P(x, y) | {}:{:>d} f(x_p_d)={:>14.9f} f(x_q)={:>14.9f} d={:>14.9f}'.format(epoch, i, fp, fq,\n",
    "                                                                                                      fp - fq))\n",
    "\n",
    "                # add to loss\n",
    "                L += args.p_x_y_weight * l_p_x_y\n",
    "            #############################\n",
    "            # End dont do this for paper\n",
    "            #############################\n",
    "\n",
    "            # break if the loss diverged...easier for poppa to run experiments this way\n",
    "            if L.abs().item() > 1e8:\n",
    "                print(\"BAD BOIIIIIIIIII\")\n",
    "                1/0\n",
    "\n",
    "            # Optimize network using our loss function L\n",
    "            optim.zero_grad()\n",
    "            L.backward()\n",
    "            optim.step()\n",
    "            cur_iter += 1\n",
    "\n",
    "            # Plot outputs\n",
    "            if cur_iter % 100 == 0:\n",
    "                if args.plot_uncond:\n",
    "                    if args.class_cond_p_x_sample:\n",
    "                        assert not args.uncond, \"can only draw class-conditional samples if EBM is class-cond\"\n",
    "                        y_q = t.randint(0, args.n_classes, (args.batch_size,)).to(device)\n",
    "                        x_q = sample_q(f, replay_buffer, y=y_q)\n",
    "                    else:\n",
    "                        x_q = sample_q(f, replay_buffer)\n",
    "                    plot('{}/x_q_{}_{:>06d}.png'.format(args.save_dir, epoch, i), x_q)\n",
    "                if args.plot_cond:  # generate class-conditional samples\n",
    "                    y = t.arange(0, args.n_classes)[None].repeat(args.n_classes, 1).transpose(1, 0).contiguous().view(-1).to(device)\n",
    "                    x_q_y = sample_q(f, replay_buffer, y=y)\n",
    "                    plot('{}/x_q_y{}_{:>06d}.png'.format(args.save_dir, epoch, i), x_q_y)\n",
    "\n",
    "        # do checkpointing\n",
    "        if epoch % args.ckpt_every == 0:\n",
    "            checkpoint(f, replay_buffer, f'ckpt_{epoch}.pt', args, device)\n",
    "\n",
    "        # Print performance assesment \n",
    "        if epoch % args.eval_every == 0 and (args.p_y_given_x_weight > 0 or args.p_x_y_weight > 0):\n",
    "            f.eval()\n",
    "            with t.no_grad():\n",
    "                # validation set\n",
    "                correct, loss = eval_classification(f, dload_valid, device)\n",
    "                print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "                if correct > best_valid_acc:\n",
    "                    best_valid_acc = correct\n",
    "                    print(\"Best Valid!: {}\".format(correct))\n",
    "                    checkpoint(f, replay_buffer, \"best_valid_ckpt.pt\", args, device)\n",
    "                # test set\n",
    "                correct, loss = eval_classification(f, dload_test, device)\n",
    "                print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "            f.train()\n",
    "            \n",
    "        # do \"last\" checkpoint\n",
    "        checkpoint(f, replay_buffer, \"last_ckpt.pt\", args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "# defaults for paper\n",
    "# --lr .0001 --dataset cifar10 --optimizer adam --p_x_weight 1.0 --p_y_given_x_weight 1.0 \n",
    "# --p_x_y_weight 0.0 --sigma .03 --width 10 --depth 28 --save_dir /YOUR/SAVE/DIR \n",
    "# --plot_uncond --warmup_iters 1000\n",
    "#\n",
    "# Regression\n",
    "# {\"dataset\": \"cifar10\", \"data_root\": \"../data\", \"lr\": 0.0001, \"decay_epochs\": [160, 180], \n",
    "# \"decay_rate\": 0.3, \"clf_only\": false, \"labels_per_class\": -1, \"optimizer\": \"adam\", \n",
    "# \"batch_size\": 64, \"n_epochs\": 200, \"warmup_iters\": 1000, \"p_x_weight\": 1.0, \n",
    "# \"p_y_given_x_weight\": 1.0, \"p_x_y_weight\": 0.0, \"dropout_rate\": 0.0, \"sigma\": 0.03, \n",
    "# \"weight_decay\": 0.0, \"norm\": null, \"n_steps\": 20, \"width\": 10, \"depth\": 28, \"uncond\": false, \n",
    "# \"class_cond_p_x_sample\": false, \"buffer_size\": 10000, \"reinit_freq\": 0.05, \"sgld_lr\": 1.0, \n",
    "# \"sgld_std\": 0.01, \"save_dir\": \"./savedir\", \"ckpt_every\": 10, \"eval_every\": 1, \n",
    "# \"print_every\": 100, \"load_path\": null, \"print_to_log\": false, \"plot_cond\": false, \n",
    "#\"plot_uncond\": true, \"n_valid\": 5000, \"n_classes\": 10}\n",
    "class train_args():\n",
    "    def __init__(self, param_dict):\n",
    "        # set defaults\n",
    "        self.dataset = \"cifar10\" #, choices=[\"cifar10\", \"svhn\", \"cifar100\"])\n",
    "        self.n_classes = 100 if self.dataset == \"cifar100\" else 10\n",
    "        self.data_root = \"../data\" \n",
    "        # optimization\n",
    "        self.lr = 1e-4\n",
    "        self.decay_epochs = [160, 180] # help=\"decay learning rate by decay_rate at these epochs\")\n",
    "        self.decay_rate = .3 # help=\"learning rate decay multiplier\")\n",
    "        self.clf_only = False #action=\"store_true\", help=\"If set, then only train the classifier\")\n",
    "        self.labels_per_class = -1# help=\"number of labeled examples per class, if zero then use all labels\")\n",
    "        self.optimizer = \"adam\" #choices=[\"adam\", \"sgd\"], default=\"adam\")\n",
    "        self.batch_size = 64\n",
    "        self.n_epochs = 200\n",
    "        self.warmup_iters = -1 # help=\"number of iters to linearly increase learning rate, if -1 then no warmmup\")\n",
    "        # loss weighting\n",
    "        self.p_x_weight = 1.\n",
    "        self.p_y_given_x_weight = 1.\n",
    "        self.p_x_y_weight = 0.\n",
    "        # regularization\n",
    "        self.dropout_rate = 0.0\n",
    "        self.sigma = 3e-2 # help=\"stddev of gaussian noise to add to input, .03 works but .1 is more stable\")\n",
    "        self.weight_decay = 0.0\n",
    "        # network\n",
    "        self.norm = None # choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"], help=\"norm to add to weights, none works fine\")\n",
    "        # EBM specific\n",
    "        self.n_steps = 20 # help=\"number of steps of SGLD per iteration, 100 works for short-run, 20 works for PCD\")\n",
    "        self.width = 10 # help=\"WRN width parameter\")\n",
    "        self.depth = 28 # help=\"WRN depth parameter\")\n",
    "        self.uncond = False # \"store_true\" # help=\"If set, then the EBM is unconditional\")\n",
    "        self.class_cond_p_x_sample = False #, action=\"store_true\", help=\"If set we sample from p(y)p(x|y), othewise sample from p(x),\" \"Sample quality higher if set, but classification accuracy better if not.\")\n",
    "        self.buffer_size = 10000\n",
    "        self.reinit_freq = .05\n",
    "        self.sgld_lr = 1.0\n",
    "        self.sgld_std = 1e-2\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.ckpt_every = 10 # help=\"Epochs between checkpoint save\")\n",
    "        self.eval_every = 1 # help=\"Epochs between evaluation\")\n",
    "        self.print_every = 100 # help=\"Iterations between print\")\n",
    "        self.load_path = None # path for checkpoint to load\n",
    "        self.print_to_log = False #\", action=\"store_true\", help=\"If true, directs std-out to log file\")\n",
    "        self.plot_cond = False #\", action=\"store_true\", help=\"If set, save class-conditional samples\")\n",
    "        self.plot_uncond = False #\", action=\"store_true\", help=\"If set, save unconditional samples\")\n",
    "        self.n_valid = 5000\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg warmup_iters 1000 lr 0.0001\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "| Wide-Resnet 28x10\n",
      "loading model from ./savedir/best_valid_ckpt.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_p_d_shape torch.Size([64, 3, 32, 32])\n",
      "x_q_shape torch.Size([64, 3, 32, 32])\n",
      "P(x) | 0:0 f(x_p_d)=   1.067865491 f(x_q)=   1.066436410 d=   0.001429081\n",
      "P(y|x) 0:0 loss=   0.336998075, acc=   0.890625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:02,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_p_d_shape torch.Size([64, 3, 32, 32])\n",
      "x_q_shape torch.Size([64, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:05,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_p_d_shape torch.Size([64, 3, 32, 32])\n",
      "x_q_shape torch.Size([64, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:08,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_p_d_shape torch.Size([64, 3, 32, 32])\n",
      "x_q_shape torch.Size([64, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:11,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_p_d_shape torch.Size([64, 3, 32, 32])\n",
      "x_q_shape torch.Size([64, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:13,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_p_d_shape torch.Size([64, 3, 32, 32])\n",
      "x_q_shape torch.Size([64, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:16,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_p_d_shape torch.Size([64, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6779329c5827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-8f036a57799e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x_p_d_shape\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_p_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mx_p_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_p_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mx_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdload_train_labeled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mx_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_lab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setup change from defaults\n",
    "# For paper defaults\n",
    "# Added best_valid_ckpt load\n",
    "inline_parms = {\"lr\": .0001, \"dataset\": \"cifar10\", \"optimizer\": \"adam\", \"save_dir\": './exp2', \\\n",
    "                \"p_x_weight\": 1.0, \"p_y_given_x_weight\": 1.0, \"p_x_y_weight\": 0.0, \\\n",
    "                \"sigma\": .03, \"width\": 10, \"depth\": 28, \"plot_uncond\": True, \"warmup_iters\": 1000, \\\n",
    "                \"load_path\": './savedir/best_valid_ckpt.pt'} # added loading from checkpoint\n",
    "\n",
    "# instantiate\n",
    "args = train_args(inline_parms)\n",
    "\n",
    "print(\"arg warmup_iters\", args.warmup_iters, \"lr\", args.lr)\n",
    "\n",
    "# run\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
