{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import utils # from The Google Research Authors\n",
    "import torch as t, torch.nn as nn, torch.nn.functional as tnnF, torch.distributions as tdist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "#import ipdb\n",
    "import numpy as np\n",
    "import wideresnet # from The Google Research Authors\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "from tqdm import tqdm\n",
    "t.backends.cudnn.benchmark = True\n",
    "t.backends.cudnn.enabled = True\n",
    "seed = 1\n",
    "\n",
    "# images RGB 32x32\n",
    "im_sz = 32\n",
    "n_ch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random subset of data\n",
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Wide_ResNet\n",
    "# Uses The Google Research Authors, file wideresnet.py\n",
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm, dropout_rate=dropout_rate)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, n_classes)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energies if y=none\n",
    "# EBM energy calculated as logsumexp of logits\n",
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm, dropout_rate=dropout_rate, n_classes=n_classes)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            # gathers the logits along dim 1 with indeces y\n",
    "            return t.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various utilities\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def grad_norm(m):\n",
    "    total_norm = 0\n",
    "    for p in m.parameters():\n",
    "        param_grad = p.grad\n",
    "        if param_grad is not None:\n",
    "            param_norm = param_grad.data.norm(2) ** 2\n",
    "            total_norm += param_norm\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm.item()\n",
    "\n",
    "def grad_vals(m):\n",
    "    ps = []\n",
    "    for p in m.parameters():\n",
    "        if p.grad is not None:\n",
    "            ps.append(p.grad.data.view(-1))\n",
    "    ps = t.cat(ps)\n",
    "    return ps.mean().item(), ps.std(), ps.abs().mean(), ps.abs().std(), ps.abs().min(), ps.abs().max()\n",
    "\n",
    "def init_random(args, bs):\n",
    "    return t.FloatTensor(bs, n_ch, im_sz, im_sz).uniform_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SGLD model and data/replay buffer\n",
    "# Images generated are added to a buffer and sampled with a probability (1-\\rho) for efficiency\n",
    "def get_model_and_buffer(args, device, sample_q):\n",
    "    model_cls = F if args.uncond else CCF\n",
    "    f = model_cls(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    if not args.uncond:\n",
    "        assert args.buffer_size % args.n_classes == 0, \"Buffer size must be divisible by args.n_classes\"\n",
    "    if args.load_path is None:\n",
    "        # make replay buffer\n",
    "        replay_buffer = init_random(args, args.buffer_size)\n",
    "        epoch=-1 #Because it needs to start at 0\n",
    "    else:\n",
    "        print(f\"loading model from {args.load_path}\")\n",
    "        ckpt_dict = t.load(args.load_path)\n",
    "        f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "        replay_buffer = ckpt_dict[\"replay_buffer\"]\n",
    "        epoch = ckpt_dict[\"epoch\"]\n",
    "\n",
    "    f = f.to(device)\n",
    "    return f, replay_buffer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in chosen dataset from svhn, cifar10, cifar100\n",
    "def get_data(args):\n",
    "    if args.dataset == \"svhn\":\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "    else:\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.RandomHorizontalFlip(),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "        #transform_train = tr.Compose(\n",
    "        #    [tr.ToTensor()]\n",
    "        #)\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "    def dataset_fn(train, transform):\n",
    "        if args.dataset == \"cifar10\":\n",
    "            return tv.datasets.CIFAR10(root=args.data_root, transform=transform, download=True, train=train)\n",
    "        elif args.dataset == \"cifar100\":\n",
    "            return tv.datasets.CIFAR100(root=args.data_root, transform=transform, download=True, train=train)\n",
    "        else:\n",
    "            return tv.datasets.SVHN(root=args.data_root, transform=transform, download=True,\n",
    "                                    split=\"train\" if train else \"test\")\n",
    "\n",
    "    # get all training inds\n",
    "    full_train = dataset_fn(True, transform_train)\n",
    "    all_inds = list(range(len(full_train)))\n",
    "    # set seed\n",
    "    np.random.seed(1234)\n",
    "    # shuffle\n",
    "    np.random.shuffle(all_inds)\n",
    "    # seperate out validation set\n",
    "    if args.n_valid is not None:\n",
    "        valid_inds, train_inds = all_inds[:args.n_valid], all_inds[args.n_valid:]\n",
    "    else:\n",
    "        valid_inds, train_inds = [], all_inds\n",
    "    train_inds = np.array(train_inds)\n",
    "    train_labeled_inds = []\n",
    "    other_inds = []\n",
    "    train_labels = np.array([full_train[ind][1] for ind in train_inds])\n",
    "    if args.labels_per_class > 0:\n",
    "        for i in range(args.n_classes):\n",
    "            print(i)\n",
    "            train_labeled_inds.extend(train_inds[train_labels == i][:args.labels_per_class])\n",
    "            other_inds.extend(train_inds[train_labels == i][args.labels_per_class:])\n",
    "    else:\n",
    "        train_labeled_inds = train_inds\n",
    "\n",
    "    dset_train = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_inds)\n",
    "    dset_train_labeled = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_labeled_inds)\n",
    "    dset_valid = DataSubset(\n",
    "        dataset_fn(True, transform_test),\n",
    "        inds=valid_inds)\n",
    "    dload_train = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = DataLoader(dset_train_labeled, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = cycle(dload_train_labeled)\n",
    "    dset_test = dataset_fn(False, transform_test)\n",
    "    dload_valid = DataLoader(dset_valid, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    dload_test = DataLoader(dset_test, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    return dload_train, dload_train_labeled, dload_valid,dload_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine for SGLD generation of fake images\n",
    "def get_sample_q(args, device):\n",
    "    # setup initial data/buffers\n",
    "    def sample_p_0(replay_buffer, bs, y=None):\n",
    "        if len(replay_buffer) == 0:\n",
    "            return init_random(args, bs), []\n",
    "        buffer_size = len(replay_buffer) if y is None else len(replay_buffer) // args.n_classes\n",
    "        inds = t.randint(0, buffer_size, (bs,))\n",
    "        # if cond, convert inds to class conditional inds\n",
    "        if y is not None:\n",
    "            inds = y.cpu() * buffer_size + inds\n",
    "            assert not args.uncond, \"Can't drawn conditional samples without giving me y\"\n",
    "        buffer_samples = replay_buffer[inds]\n",
    "        random_samples = init_random(args, bs)\n",
    "        choose_random = (t.rand(bs) < args.reinit_freq).float()[:, None, None, None]\n",
    "        samples = choose_random * random_samples + (1 - choose_random) * buffer_samples\n",
    "        return samples.to(device), inds\n",
    "\n",
    "    # actual SGLD\n",
    "    def sample_q(f, replay_buffer, y=None, n_steps=args.n_steps):\n",
    "        \"\"\"this func takes in replay_buffer now so we have the option to sample from\n",
    "        scratch (i.e. replay_buffer==[]).  See test_wrn_ebm.py for example.\n",
    "        \"\"\"\n",
    "        # here f is CCF to calculate energies\n",
    "        # evaluate model, must set train back on later (TODO:but I dont need to train energies?)\n",
    "        f.eval()\n",
    "        # get batch size\n",
    "        bs = args.batch_size if y is None else y.size(0)\n",
    "        # generate initial samples and buffer inds of those samples (if buffer is used)\n",
    "        init_sample, buffer_inds = sample_p_0(replay_buffer, bs=bs, y=y)\n",
    "        x_k = t.autograd.Variable(init_sample, requires_grad=True)\n",
    "        # sgld\n",
    "        for k in range(n_steps):\n",
    "            # calculate \\parial E/\\partial x_{k-1}\n",
    "            f_prime = t.autograd.grad(f(x_k, y=y).sum(), [x_k], retain_graph=True)[0]\n",
    "            # x_k = x_{k-1} + \\alpha*\\parial E/\\partial x_{k-1} + \\theta * N\n",
    "            x_k.data += args.sgld_lr * f_prime + args.sgld_std * t.randn_like(x_k)\n",
    "        \n",
    "        # set self.training = True\n",
    "        f.train()\n",
    "        \n",
    "        # Returns a new Tensor, detached from the current graph\n",
    "        final_samples = x_k.detach()\n",
    "        \n",
    "        # update replay buffer\n",
    "        if len(replay_buffer) > 0:\n",
    "            replay_buffer[buffer_inds] = final_samples.cpu()\n",
    "        return final_samples\n",
    "    return sample_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid repeat code and maintanence. This is for the evaluations\n",
    "def eval_classification_inner(f,dload,device):\n",
    "    softmax=nn.Softmax(dim=1)\n",
    "    corrects, losses, logits_all = [], [], []\n",
    "    for x_p_d, y_p_d in dload:\n",
    "        x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "        logits = f.classify(x_p_d)\n",
    "        logits_all.extend(logits)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().numpy()\n",
    "        losses.extend(loss)\n",
    "\n",
    "        correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "        corrects.extend(correct)\n",
    "\n",
    "    logits_all=t.stack(logits_all)\n",
    "    logits=softmax(logits_all)\n",
    "    sms = logits.max(1)[0]\n",
    "    cali_vals=[(a,b.item()) for a,b in zip(corrects,sms)]\n",
    "    return corrects, losses, cali_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss and accuracy for periodic printout\n",
    "def eval_classification(f, dload, device):\n",
    "    corrects, losses, _ = eval_classification_inner(f,dload,device)\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    return correct, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the calibration data to a file\n",
    "def save_calibration(filename,cali_vals):\n",
    "    with open(filename,\"w\") as f:\n",
    "        f.write(\"correct,softmax\\n\")\n",
    "        for i in cali_vals:\n",
    "            f.write(\"{},{}\\n\".format(i[0],i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate loss and accuracy for calibration\n",
    "def eval_with_calibration(f, dload, device):\n",
    "    corrects, losses, cali_vals = eval_classification_inner(f,dload,device)\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    save_calibration(os.path.join(args.save_dir,f'cali_{ev}.csv'),cali_vals)\n",
    "    return correct, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track loss for convergence\n",
    "def loss_tracker(filename,epoch,loss,correct):\n",
    "    if not os.path.isfile(os.path.join(args.save_dir,filename)):\n",
    "        with open(os.path.join(args.save_dir,filename),'w') as of:\n",
    "            of.write(\"Epoch,Loss,Acc\\n\")\n",
    "            of.write(\"{},{},{}\\n\".format(epoch,loss,correct))\n",
    "    else:\n",
    "        with open(os.path.join(args.save_dir,filename),'a') as of:\n",
    "            of.write(\"{},{},{}\\n\".format(epoch,loss,correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint data\n",
    "def checkpoint(f, opt, buffer, epoch_no, tag, args, device):\n",
    "    f.cpu()\n",
    "    ckpt_dict = {\n",
    "        \"model_state_dict\": f.state_dict(),\n",
    "        'optimizer_state_dict': opt.state_dict(),\n",
    "        'epoch': epoch_no,\n",
    "        \"replay_buffer\": buffer\n",
    "    }\n",
    "    t.save(ckpt_dict, os.path.join(args.save_dir, tag))\n",
    "    t.save(ckpt_dict, os.path.join(args.save_dir,'most_recent.pt'))\n",
    "    f.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track loss for convergence\n",
    "def loss_tracker(filename,save_dir,epoch,loss,correct):\n",
    "    if not os.path.isfile(os.path.join(args.save_dir,filename)):\n",
    "        with open(os.path.join(args.save_dir,filename),'w') as f:\n",
    "            f.write(\"Epoch,Loss,Acc\\n\")\n",
    "            f.write(\"{},{},{}\\n\".format(epoch,loss,correct))\n",
    "    else:\n",
    "        with open(os.path.join(args.save_dir,filename),'a') as f:\n",
    "            f.write(\"{},{},{}\\n\".format(epoch,loss,correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the newest ckpt if not using the \"most_recent.pt\" file\n",
    "def nat_keys(word):\n",
    "    def atoi(c):\n",
    "        return int(c) if c.isdigit() else c\n",
    "    return [atoi(c) for c in re.split('(\\d+)',word)]\n",
    "\n",
    "def get_most_recent_ckpt(dir):\n",
    "    ckpt=sorted([i for i in os.listdir(dir) if 'ckpt'==i[:4]],key=nat_keys)[-1]\n",
    "    return os.path.join(dir,ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function adds or overwrites a file to the output dir named '0_readme.txt'\n",
    "#That file contains what we were hoping to do with that experiment\n",
    "def exp_purpose(words,filename='0_readme.txt'):\n",
    "    with open(os.path.join(args.save_dir,filename),'w') as f:\n",
    "        f.write(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(args,f):\n",
    "    params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "    if args.optimizer == \"adam\":\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        optim = t.optim.SGD(params, lr=args.lr, momentum=.9, weight_decay=args.weight_decay)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_experiment(args,seed):\n",
    "    utils.makedirs(args.save_dir)\n",
    "    with open(f'{args.save_dir}/params.txt', 'w') as f:\n",
    "        json.dump(args.__dict__, f)\n",
    "    if args.print_to_log:\n",
    "        sys.stdout = open(f'{args.save_dir}/log.txt', 'w')\n",
    "\n",
    "    t.manual_seed(seed)\n",
    "    if t.cuda.is_available():\n",
    "        t.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    # store purpose of experiment\n",
    "    exp_purpose(\"Get SGLD to train using the CCF model and simplest parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterify(var):\n",
    "    if type(var)==str:\n",
    "        return [var]\n",
    "    try:\n",
    "        iter(var)\n",
    "    except:\n",
    "        var=[var]\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I tested this, it does not need to return to update the optimizer\n",
    "def decay_epoch(optim):\n",
    "    for param_group in optim.param_groups:\n",
    "        new_lr = param_group['lr'] * args.decay_rate\n",
    "        param_group['lr'] = new_lr\n",
    "    print(\"Decaying lr to {}\".format(new_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_epoch(optim,cur_iter):\n",
    "    lr = args.lr * cur_iter / float(args.warmup_iters)\n",
    "    for param_group in optim.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_ent(f,x_lab,y_lab,epoch,cur_iter):\n",
    "    logits = f.classify(x_lab)\n",
    "    l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "    if cur_iter % args.print_every == 0:\n",
    "        acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "        print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,cur_iter,\n",
    "                                                                     l_p_y_given_x.item(),acc.item()))\n",
    "    return l_p_y_given_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_paper(sample_q,f,replay_buffer,y_lab,x_lab):\n",
    "    assert not args.uncond, \"this objective can only be trained for class-conditional EBM DUUUUUUUUHHHH!!!\"\n",
    "    x_q_lab = sample_q(f, replay_buffer, y=y_lab)\n",
    "    fp, fq = f(x_lab, y_lab).mean(), f(x_q_lab, y_lab).mean()\n",
    "    l_p_x_y = -(fp - fq)\n",
    "    if cur_iter % args.print_every == 0:\n",
    "        print('P(x, y) | {}:{:>d} f(x_p_d)={:>14.9f} f(x_q)={:>14.9f} d={:>14.9f}'.format(epoch, i, fp, fq,fp-fq))\n",
    "    return l_p_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for training\n",
    "# Uses args from class below\n",
    "def main(args):\n",
    "    ######################################################\n",
    "    ###                                                ###\n",
    "    ###               Closure functions                ###\n",
    "    ###                                                ###\n",
    "    ######################################################\n",
    "    \n",
    "    #Three functions for the evaluation.              \n",
    "    def basic_eval(eval_func,dls,evs=None,with_tracker=False):\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            for ev,dl in zip(iterify(evs),iterify(dls)):\n",
    "                print('ev: ',ev)\n",
    "                correct, loss = eval_func(f, dload_test, device)    \n",
    "                if with_tracker:\n",
    "                    loss_tracker(f'track_{ev}.csv',args.save_dir,epoch,loss,correct)\n",
    "        print(f\"{ev}: Epoch {epoch}: Valid Loss {loss}, Valid Acc {correct}\")\n",
    "        f.train()\n",
    "        return correct\n",
    "    \n",
    "    def eval_all_3(eval_func,with_tracker=False):\n",
    "        evs=['test', 'train', 'valid']\n",
    "        dls=[dload_test,dload_train,dload_valid]\n",
    "        return basic_eval(eval_func,dls,evs,with_tracker)\n",
    "    \n",
    "    def update_best():\n",
    "        print(\"Best Valid!: {}\".format(correct))\n",
    "        checkpoint(f, optim, replay_buffer, epoch, f'best_valid_ckpt.pt', args, device)\n",
    "    \n",
    "    #Loss options\n",
    "    def sgld():\n",
    "        if args.class_cond_p_x_sample:\n",
    "            assert not args.uncond, \"can only draw class-conditional samples if EBM is class-cond\"\n",
    "            y_q = t.randint(0, args.n_classes, (args.batch_size,)).to(device)\n",
    "            x_q = sample_q(f, replay_buffer, y=y_q)\n",
    "        else:\n",
    "            # get data generated by SGLD\n",
    "            # In paper x_q_shape torch.Size([64, 3, 32, 32])\n",
    "            # Batch rgb 32x32\n",
    "            x_q = sample_q(f, replay_buffer)  # sample from log-sumexp\n",
    "            #print(\"x_q_shape\",x_q.shape)\n",
    "\n",
    "        # calculate energy for training data\n",
    "        fp_all = f(x_p_d)\n",
    "\n",
    "        # calculate energy for SGLD generated sample\n",
    "        fq_all = f(x_q)\n",
    "\n",
    "        # get means\n",
    "        fp = fp_all.mean()\n",
    "        fq = fq_all.mean()\n",
    "\n",
    "        # surrogate for the difference of expected value of \\partial Energy/\\partial x\n",
    "        # and \\partial Energy/\\partial x\n",
    "        # Need to maximize this, so preceded by minus\n",
    "        l_p_x = -(fp - fq)\n",
    "        if cur_iter % args.print_every == 0:\n",
    "            print('P(x) | {}:{:>d} f(x_p_d)={:>14.9f} f(x_q)={:>14.9f} d={:>14.9f}'.format(epoch, i, fp, fq,fp - fq))\n",
    "\n",
    "        return l_p_x    \n",
    "    \n",
    "    #Two functions for the adaptive learning\n",
    "    def retry_epoch():\n",
    "        bad_epoch=epoch\n",
    "        args.sgld_lr/=2\n",
    "        args.load_path=os.path.join(args.save_dir,f'ckpt_{(epoch-1)}.pt')\n",
    "        f, replay_buffer, _ = get_model_and_buffer(args, device, sample_q)\n",
    "        print(f'Diverged: Using adaptive learning: ckpt_{(epoch-1)}.pt')\n",
    "        print(f'New sgld_lr: {args.sgld_lr}')\n",
    "    \n",
    "    def restore_lr():\n",
    "        args.sgld_lr=org_sgld_lr\n",
    "        print(\"Adaptive learning over, restored original lrs.\")\n",
    "        \n",
    "    #I just moved this code wholesale to get it out of my way\n",
    "    def handle_plots():\n",
    "        if cur_iter % 100 == 0:\n",
    "            if args.plot_uncond:\n",
    "                if args.class_cond_p_x_sample:\n",
    "                    assert not args.uncond, \"can only draw class-conditional samples if EBM is class-cond\"\n",
    "                    y_q = t.randint(0, args.n_classes, (args.batch_size,)).to(device)\n",
    "                    x_q = sample_q(f, replay_buffer, y=y_q)\n",
    "                else:\n",
    "                    x_q = sample_q(f, replay_buffer)\n",
    "                plot('{}/x_q_{}_{:>06d}.png'.format(args.save_dir, epoch, i), x_q)\n",
    "            if args.plot_cond:  # generate class-conditional samples\n",
    "                y = t.arange(0, args.n_classes)[None].repeat(args.n_classes, 1).transpose(1, 0).contiguous().view(-1).to(device)\n",
    "                x_q_y = sample_q(f, replay_buffer, y=y)\n",
    "                plot('{}/x_q_y{}_{:>06d}.png'.format(args.save_dir, epoch, i), x_q_y)\n",
    "      \n",
    "    \n",
    "    \n",
    "    ######################################################\n",
    "    ###                                                ###\n",
    "    ###                  Start main                    ###\n",
    "    ###                                                ###\n",
    "    ######################################################\n",
    "    \n",
    "    \n",
    "    set_up_experiment(args, seed)\n",
    "    \n",
    "    # datasets\n",
    "    dload_train, dload_train_labeled, dload_valid, dload_test = get_data(args)\n",
    "\n",
    "    # device\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # MODEL\n",
    "    sample_q = get_sample_q(args, device)\n",
    "    f, replay_buffer, epoch = get_model_and_buffer(args, device, sample_q)\n",
    "\n",
    "    sqrt = lambda x: int(t.sqrt(t.Tensor([x])))\n",
    "    plot = lambda p, x: tv.utils.save_image(t.clamp(x, -1, 1), p, normalize=True, nrow=sqrt(x.size(0)))\n",
    "\n",
    "    # optimizer\n",
    "    optim=get_optimizer(args,f)\n",
    "\n",
    "    # Quick eval of imported model\n",
    "    basic_eval(eval_classification,dload_valid,'valid')\n",
    "    \n",
    "    #Set variables for the while loop\n",
    "    bad_epoch=-1\n",
    "    best_valid_acc = 0.0\n",
    "    cur_iter = 0\n",
    "    epoch+=1\n",
    "    final_epoch=args.n_epochs+epoch\n",
    "    diverged=False\n",
    "    org_lr=args.lr\n",
    "    org_sgld_lr=args.sgld_lr\n",
    "    \n",
    "    # loop over epochs -> While loop so we can go back epochs\n",
    "    while epoch<final_epoch:\n",
    "        \n",
    "        # decaying learning rate?\n",
    "        if (epoch in args.decay_epochs) and (not diverged): decay_epoch(optim)\n",
    "                       \n",
    "        # loop over data in batches\n",
    "        # x_p_d sample from dataset\n",
    "        for i, (x_p_d, _) in tqdm(enumerate(dload_train)):\n",
    "            # scale up lr to full over warmup time\n",
    "            if cur_iter <= args.warmup_iters: warmup_epoch(optim,cur_iter)\n",
    "                \n",
    "            x_p_d = x_p_d.to(device)\n",
    "            x_lab, y_lab = dload_train_labeled.__next__()\n",
    "            x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "            # initialize loss\n",
    "            L = 0.\n",
    "            \n",
    "            # this maximizes log p(x) using SGLD\n",
    "            if args.p_x_weight > 0:  # maximize log p(x)\n",
    "                l_p_x = sgld()                                                                                  \n",
    "                # add to loss\n",
    "                L += args.p_x_weight * l_p_x\n",
    "\n",
    "            # normal cross entropy loss function\n",
    "            if args.p_y_given_x_weight > 0:  # maximize log p(y | x)\n",
    "                l_p_y_given_x=x_ent(f,x_lab,y_lab,epoch,cur_iter)\n",
    "                # add to loss\n",
    "                L += args.p_y_given_x_weight * l_p_y_given_x\n",
    "            \n",
    "            #The code not for the paper\n",
    "            if args.p_x_y_weight > 0:  # maximize log p(x, y)\n",
    "                l_p_x_y = not_paper(sample_q,f,replay_buffer,y_lab,x_lab)\n",
    "                # add to loss\n",
    "                L += args.p_x_y_weight * l_p_x_y\n",
    "\n",
    "            # Handle Loss divergence\n",
    "            if L.abs().item() > 1e8:\n",
    "                retry_epoch()\n",
    "                diverged=True\n",
    "                break\n",
    "            \n",
    "            # Optimize network using our loss function L\n",
    "            optim.zero_grad()            \n",
    "            L.backward()\n",
    "            optim.step()\n",
    "            cur_iter += 1\n",
    "\n",
    "            # Plot outputs\n",
    "            handle_plots()\n",
    "\n",
    "        ####### END FOR LOOP \n",
    "        \n",
    "        # restore after bad epoch\n",
    "        if diverged and epoch>bad_epoch:\n",
    "            restore_lr()\n",
    "            diverged=False\n",
    "            \n",
    "        # If it diverged, then skip the evaluation and don't increment epoch\n",
    "        if not diverged:        \n",
    "            # Checkpoint\n",
    "            if epoch % args.ckpt_every == 0: \n",
    "                checkpoint(f, optim, replay_buffer, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "\n",
    "            # Performance assesment \n",
    "            if epoch % args.eval_every == 0 and (args.p_y_given_x_weight > 0 or args.p_x_y_weight > 0):\n",
    "                correct = eval_all_3(eval_classification,with_tracker=True)\n",
    "                if correct > best_valid_acc: \n",
    "                    best_valid_acc = correct\n",
    "                    update_best()\n",
    " \n",
    "            epoch+=1\n",
    "\n",
    "    ####### END WHILE LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "# defaults for paper\n",
    "# --lr .0001 --dataset cifar10 --optimizer adam --p_x_weight 1.0 --p_y_given_x_weight 1.0 \n",
    "# --p_x_y_weight 0.0 --sigma .03 --width 10 --depth 28 --save_dir /YOUR/SAVE/DIR \n",
    "# --plot_uncond --warmup_iters 1000\n",
    "#\n",
    "# Regression\n",
    "# {\"dataset\": \"cifar10\", \"data_root\": \"../data\", \"lr\": 0.0001, \"decay_epochs\": [160, 180], \n",
    "# \"decay_rate\": 0.3, \"clf_only\": false, \"labels_per_class\": -1, \"optimizer\": \"adam\", \n",
    "# \"batch_size\": 64, \"n_epochs\": 200, \"warmup_iters\": 1000, \"p_x_weight\": 1.0, \n",
    "# \"p_y_given_x_weight\": 1.0, \"p_x_y_weight\": 0.0, \"dropout_rate\": 0.0, \"sigma\": 0.03, \n",
    "# \"weight_decay\": 0.0, \"norm\": null, \"n_steps\": 20, \"width\": 10, \"depth\": 28, \"uncond\": false, \n",
    "# \"class_cond_p_x_sample\": false, \"buffer_size\": 10000, \"reinit_freq\": 0.05, \"sgld_lr\": 1.0, \n",
    "# \"sgld_std\": 0.01, \"save_dir\": \"./savedir\", \"ckpt_every\": 10, \"eval_every\": 1, \n",
    "# \"print_every\": 100, \"load_path\": null, \"print_to_log\": false, \"plot_cond\": false, \n",
    "#\"plot_uncond\": true, \"n_valid\": 5000, \"n_classes\": 10}\n",
    "class train_args():\n",
    "    def __init__(self, param_dict):\n",
    "        # set defaults\n",
    "        self.dataset = \"cifar10\" #, choices=[\"cifar10\", \"svhn\", \"cifar100\"])\n",
    "        self.n_classes = 100 if self.dataset == \"cifar100\" else 10\n",
    "        self.data_root = \"../data\" \n",
    "        # optimization\n",
    "        self.lr = 1e-4\n",
    "        self.decay_epochs = [160, 180] # help=\"decay learning rate by decay_rate at these epochs\")\n",
    "        self.decay_rate = .3 # help=\"learning rate decay multiplier\")\n",
    "        self.clf_only = False #action=\"store_true\", help=\"If set, then only train the classifier\")\n",
    "        self.labels_per_class = -1# help=\"number of labeled examples per class, if zero then use all labels\")\n",
    "        self.optimizer = \"adam\" #choices=[\"adam\", \"sgd\"], default=\"adam\")\n",
    "        self.batch_size = 64\n",
    "        self.n_epochs = 200\n",
    "        self.warmup_iters = -1 # help=\"number of iters to linearly increase learning rate, if -1 then no warmmup\")\n",
    "        # loss weighting\n",
    "        self.p_x_weight = 1.\n",
    "        self.p_y_given_x_weight = 1.\n",
    "        self.p_x_y_weight = 0.\n",
    "        # regularization\n",
    "        self.dropout_rate = 0.0\n",
    "        self.sigma = 3e-2 # help=\"stddev of gaussian noise to add to input, .03 works but .1 is more stable\")\n",
    "        self.weight_decay = 0.0\n",
    "        # network\n",
    "        self.norm = None # choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"], help=\"norm to add to weights, none works fine\")\n",
    "        # EBM specific\n",
    "        self.n_steps = 20 # help=\"number of steps of SGLD per iteration, 100 works for short-run, 20 works for PCD\")\n",
    "        self.width = 10 # help=\"WRN width parameter\")\n",
    "        self.depth = 28 # help=\"WRN depth parameter\")\n",
    "        self.uncond = False # \"store_true\" # help=\"If set, then the EBM is unconditional\")\n",
    "        self.class_cond_p_x_sample = False #, action=\"store_true\", help=\"If set we sample from p(y)p(x|y), othewise sample from p(x),\" \"Sample quality higher if set, but classification accuracy better if not.\")\n",
    "        self.buffer_size = 10000\n",
    "        self.reinit_freq = .05\n",
    "        self.sgld_lr = 1.0\n",
    "        self.sgld_std = 1e-2\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.ckpt_every = 10 # help=\"Epochs between checkpoint save\")\n",
    "        self.eval_every = 1 # help=\"Epochs between evaluation\")\n",
    "        self.print_every = 100 # help=\"Iterations between print\")\n",
    "        self.load_path = None # path for checkpoint to load\n",
    "        self.print_to_log = False #\", action=\"store_true\", help=\"If true, directs std-out to log file\")\n",
    "        self.plot_cond = False #\", action=\"store_true\", help=\"If set, save class-conditional samples\")\n",
    "        self.plot_uncond = False #\", action=\"store_true\", help=\"If set, save unconditional samples\")\n",
    "        self.n_valid = 5000\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg warmup_iters -1 lr 0.0001\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "| Wide-Resnet 28x10\n",
      "ev:  valid\n",
      "valid: Epoch -1: Valid Loss 2.303565740585327, Valid Acc 0.10790000110864639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 0:0 f(x_p_d)=   2.309696674 f(x_q)=   2.309451103 d=   0.000245571\n",
      "P(y|x) 0:0 loss=   2.303762197, acc=   0.109375000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [04:42,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 0:100 f(x_p_d)=   9.939348221 f(x_q)=  11.122454643 d=  -1.183106422\n",
      "P(y|x) 0:100 loss=   2.199460030, acc=   0.187500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [09:47,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 0:200 f(x_p_d)=   0.367403984 f(x_q)=  -0.175093085 d=   0.542497039\n",
      "P(y|x) 0:200 loss=   2.223426580, acc=   0.171875000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [14:27,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 0:300 f(x_p_d)=   2.501058102 f(x_q)=   2.473607063 d=   0.027451038\n",
      "P(y|x) 0:300 loss=   2.088834763, acc=   0.218750000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [19:07,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 0:400 f(x_p_d)=  11.440461159 f(x_q)=  11.446583748 d=  -0.006122589\n",
      "P(y|x) 0:400 loss=   2.284358263, acc=   0.140625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [23:48,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 0:500 f(x_p_d)=   4.173076630 f(x_q)=   4.192186356 d=  -0.019109726\n",
      "P(y|x) 0:500 loss=   1.820779204, acc=   0.296875000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [28:28,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 0:600 f(x_p_d)=   3.971379757 f(x_q)=   3.993027449 d=  -0.021647692\n",
      "P(y|x) 0:600 loss=   1.956935883, acc=   0.265625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "701it [33:08,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 0:700 f(x_p_d)=   3.414739132 f(x_q)=   3.606391430 d=  -0.191652298\n",
      "P(y|x) 0:700 loss=   1.916709542, acc=   0.250000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "703it [33:14,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ev:  test\n",
      "ev:  train\n",
      "ev:  valid\n",
      "valid: Epoch 0: Valid Loss 1.766089677810669, Valid Acc 0.3695000112056732\n",
      "correct:  0.3695   bv:  0.0\n",
      "Best Valid!: 0.3695000112056732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [04:34,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 1:97 f(x_p_d)=   4.657017708 f(x_q)=   4.282540798 d=   0.374476910\n",
      "P(y|x) 1:800 loss=   1.770843029, acc=   0.406250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "197it [09:20,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 1:197 f(x_p_d)=   2.571086407 f(x_q)=   2.537986040 d=   0.033100367\n",
      "P(y|x) 1:900 loss=   1.677039623, acc=   0.390625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "298it [14:19,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 1:297 f(x_p_d)=   2.950255156 f(x_q)=   3.119127750 d=  -0.168872595\n",
      "P(y|x) 1:1000 loss=   1.533204436, acc=   0.421875000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398it [20:15,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 1:397 f(x_p_d)=   4.228268147 f(x_q)=   4.239140511 d=  -0.010872364\n",
      "P(y|x) 1:1100 loss=   1.872013330, acc=   0.281250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "498it [25:03,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 1:497 f(x_p_d)=   2.946261644 f(x_q)=   3.152566433 d=  -0.206304789\n",
      "P(y|x) 1:1200 loss=   1.430279613, acc=   0.609375000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "598it [30:25,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 1:597 f(x_p_d)=   2.703156471 f(x_q)=   2.728488207 d=  -0.025331736\n",
      "P(y|x) 1:1300 loss=   1.508767843, acc=   0.406250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "698it [35:52,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 1:697 f(x_p_d)=   3.784632921 f(x_q)=   3.656646013 d=   0.127986908\n",
      "P(y|x) 1:1400 loss=   1.680570841, acc=   0.390625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "703it [36:06,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ev:  test\n",
      "ev:  train\n",
      "ev:  valid\n",
      "valid: Epoch 1: Valid Loss 1.6045269966125488, Valid Acc 0.42100000381469727\n",
      "correct:  0.421   bv:  0.3695\n",
      "Best Valid!: 0.42100000381469727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [04:26,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 2:94 f(x_p_d)=   2.822240114 f(x_q)=   2.909467936 d=  -0.087227821\n",
      "P(y|x) 2:1500 loss=   1.617092252, acc=   0.468750000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "195it [09:53,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 2:194 f(x_p_d)=   2.376142979 f(x_q)=   2.536859751 d=  -0.160716772\n",
      "P(y|x) 2:1600 loss=   1.462642193, acc=   0.562500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "295it [15:19,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 2:294 f(x_p_d)=   2.323877811 f(x_q)=   2.495153427 d=  -0.171275616\n",
      "P(y|x) 2:1700 loss=   1.589233160, acc=   0.421875000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "495it [24:40,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 2:494 f(x_p_d)=   2.197907925 f(x_q)=   2.317839146 d=  -0.119931221\n",
      "P(y|x) 2:1900 loss=   1.339205027, acc=   0.468750000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "595it [29:20,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 2:594 f(x_p_d)=   2.355030537 f(x_q)=   2.364267349 d=  -0.009236813\n",
      "P(y|x) 2:2000 loss=   1.616023898, acc=   0.406250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "695it [34:00,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 2:694 f(x_p_d)=   2.242110252 f(x_q)=   2.367894173 d=  -0.125783920\n",
      "P(y|x) 2:2100 loss=   1.481277227, acc=   0.453125000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "703it [34:23,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ev:  test\n",
      "ev:  train\n",
      "ev:  valid\n",
      "valid: Epoch 2: Valid Loss 1.3733714818954468, Valid Acc 0.4950000047683716\n",
      "correct:  0.495   bv:  0.421\n",
      "Best Valid!: 0.4950000047683716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [04:17,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 3:91 f(x_p_d)=   2.500199795 f(x_q)=   2.498788357 d=   0.001411438\n",
      "P(y|x) 3:2200 loss=   1.377061367, acc=   0.484375000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192it [08:57,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 3:191 f(x_p_d)=   2.300653219 f(x_q)=   2.522956371 d=  -0.222303152\n",
      "P(y|x) 3:2300 loss=   1.376316547, acc=   0.453125000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "292it [13:37,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 3:291 f(x_p_d)=   2.107603073 f(x_q)=   2.305500269 d=  -0.197897196\n",
      "P(y|x) 3:2400 loss=   1.121278286, acc=   0.640625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "392it [18:17,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 3:391 f(x_p_d)=   1.968465209 f(x_q)=   2.170437098 d=  -0.201971889\n",
      "P(y|x) 3:2500 loss=   1.352678299, acc=   0.515625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "492it [22:57,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 3:491 f(x_p_d)=   2.247944593 f(x_q)=   2.286842346 d=  -0.038897753\n",
      "P(y|x) 3:2600 loss=   1.272307158, acc=   0.500000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "592it [27:37,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 3:591 f(x_p_d)=   2.003799677 f(x_q)=   2.009208441 d=  -0.005408764\n",
      "P(y|x) 3:2700 loss=   1.300361156, acc=   0.437500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "692it [32:17,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 3:691 f(x_p_d)=   1.974152207 f(x_q)=   2.102665663 d=  -0.128513455\n",
      "P(y|x) 3:2800 loss=   1.161562920, acc=   0.640625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "703it [32:48,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ev:  test\n",
      "ev:  train\n",
      "ev:  valid\n",
      "valid: Epoch 3: Valid Loss 1.242893099784851, Valid Acc 0.5619000196456909\n",
      "correct:  0.5619   bv:  0.495\n",
      "Best Valid!: 0.5619000196456909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [04:09,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 4:88 f(x_p_d)=   1.982879162 f(x_q)=   2.024422884 d=  -0.041543722\n",
      "P(y|x) 4:2900 loss=   1.031087160, acc=   0.687500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "189it [08:50,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 4:188 f(x_p_d)=   1.873187780 f(x_q)=   2.007177591 d=  -0.133989811\n",
      "P(y|x) 4:3000 loss=   1.223936677, acc=   0.437500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "289it [13:30,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 4:288 f(x_p_d)=   1.979561687 f(x_q)=   2.032884598 d=  -0.053322911\n",
      "P(y|x) 4:3100 loss=   1.043070316, acc=   0.625000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "389it [18:10,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 4:388 f(x_p_d)=   1.943130732 f(x_q)=   1.822486877 d=   0.120643854\n",
      "P(y|x) 4:3200 loss=   1.030265331, acc=   0.625000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "489it [22:50,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 4:488 f(x_p_d)=   1.682754159 f(x_q)=   1.958419323 d=  -0.275665164\n",
      "P(y|x) 4:3300 loss=   1.073042035, acc=   0.562500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "589it [27:30,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 4:588 f(x_p_d)=   1.572268963 f(x_q)=   1.608330250 d=  -0.036061287\n",
      "P(y|x) 4:3400 loss=   1.142760038, acc=   0.578125000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "689it [32:10,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 4:688 f(x_p_d)=   1.770121098 f(x_q)=   1.972801924 d=  -0.202680826\n",
      "P(y|x) 4:3500 loss=   1.008027077, acc=   0.687500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "703it [32:49,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ev:  test\n",
      "ev:  train\n",
      "ev:  valid\n",
      "valid: Epoch 4: Valid Loss 0.9892320036888123, Valid Acc 0.6466000080108643\n",
      "correct:  0.6466   bv:  0.5619\n",
      "Best Valid!: 0.6466000080108643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [04:01,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 5:85 f(x_p_d)=   1.799962282 f(x_q)=   1.913630009 d=  -0.113667727\n",
      "P(y|x) 5:3600 loss=   1.002314806, acc=   0.656250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [08:41,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 5:185 f(x_p_d)=   1.268402338 f(x_q)=   1.419308066 d=  -0.150905728\n",
      "P(y|x) 5:3700 loss=   0.882778108, acc=   0.718750000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "286it [13:21,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 5:285 f(x_p_d)=   1.588144779 f(x_q)=   1.718800664 d=  -0.130655885\n",
      "P(y|x) 5:3800 loss=   1.114893794, acc=   0.609375000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "386it [18:02,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 5:385 f(x_p_d)=   1.374100208 f(x_q)=   1.542174816 d=  -0.168074608\n",
      "P(y|x) 5:3900 loss=   0.939310551, acc=   0.671875000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "486it [22:42,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 5:485 f(x_p_d)=   1.414124370 f(x_q)=   1.506900072 d=  -0.092775702\n",
      "P(y|x) 5:4000 loss=   0.977441967, acc=   0.687500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "586it [27:23,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 5:585 f(x_p_d)=   1.580553532 f(x_q)=   1.716961145 d=  -0.136407614\n",
      "P(y|x) 5:4100 loss=   0.717349946, acc=   0.781250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "686it [32:04,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 5:685 f(x_p_d)=   1.166516304 f(x_q)=   1.316848755 d=  -0.150332451\n",
      "P(y|x) 5:4200 loss=   0.977720380, acc=   0.671875000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "703it [32:51,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ev:  test\n",
      "ev:  train\n",
      "ev:  valid\n",
      "valid: Epoch 5: Valid Loss 0.8444145321846008, Valid Acc 0.7113000154495239\n",
      "correct:  0.7113   bv:  0.6466\n",
      "Best Valid!: 0.7113000154495239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [03:52,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 6:82 f(x_p_d)=   1.565459013 f(x_q)=   1.656961560 d=  -0.091502547\n",
      "P(y|x) 6:4300 loss=   0.848205268, acc=   0.734375000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "183it [08:32,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 6:182 f(x_p_d)=   1.459633946 f(x_q)=   1.817513704 d=  -0.357879758\n",
      "P(y|x) 6:4400 loss=   1.044182420, acc=   0.625000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "283it [13:12,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 6:282 f(x_p_d)=   1.545941830 f(x_q)=   1.570677757 d=  -0.024735928\n",
      "P(y|x) 6:4500 loss=   0.903640330, acc=   0.656250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383it [17:52,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 6:382 f(x_p_d)=   1.614264011 f(x_q)=   1.733219981 d=  -0.118955970\n",
      "P(y|x) 6:4600 loss=   0.939245939, acc=   0.687500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "483it [22:31,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 6:482 f(x_p_d)=   1.622172475 f(x_q)=   1.695584774 d=  -0.073412299\n",
      "P(y|x) 6:4700 loss=   0.781305075, acc=   0.734375000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "537it [25:02,  2.80s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "659it [30:48,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 14:658 f(x_p_d)=   1.126653433 f(x_q)=   1.084099531 d=   0.042553902\n",
      "P(y|x) 14:10500 loss=   0.364027560, acc=   0.859375000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "703it [32:51,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ev:  test\n",
      "ev:  train\n",
      "ev:  valid\n",
      "valid: Epoch 14: Valid Loss 0.3908502161502838, Valid Acc 0.8701000213623047\n",
      "correct:  0.8701   bv:  0.8466\n",
      "Best Valid!: 0.8701000213623047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [02:37,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 15:55 f(x_p_d)=   0.977245450 f(x_q)=   1.063592911 d=  -0.086347461\n",
      "P(y|x) 15:10600 loss=   0.174873531, acc=   0.953125000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [07:17,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x) | 15:155 f(x_p_d)=   1.093029737 f(x_q)=   1.145773411 d=  -0.052743673\n",
      "P(y|x) 15:10700 loss=   0.255814075, acc=   0.890625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [11:40,  2.80s/it]"
     ]
    }
   ],
   "source": [
    "# setup change from defaults\n",
    "inline_parms = {\"lr\": .0001, \"dataset\": \"cifar10\", \"optimizer\": \"adam\", \n",
    "                \"save_dir\": './test_X-ent_SGLD_refactored/', \\\n",
    "                \"p_x_weight\": 1.0, \"p_y_given_x_weight\": 1.0, \"p_x_y_weight\": 0.0, \\\n",
    "                \"sigma\": .03, \"width\": 10, \"depth\": 28, \"plot_uncond\": False, \\\n",
    "                \"uncond\": False, \"decay_epochs\": [], \\\n",
    "                \"ckpt_every\": 1, \\\n",
    "                 \"n_epochs\": 100} \n",
    "\n",
    "# instantiate\n",
    "args = train_args(inline_parms)\n",
    "\n",
    "print(\"arg warmup_iters\", args.warmup_iters, \"lr\", args.lr)\n",
    "\n",
    "# run\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
